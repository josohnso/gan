{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of make_LANDSCAPE_pretrain_Generator_v0.4.1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# target\n",
        "target = 'landscape'\n",
        "target_JP = '風景'\n",
        "\n",
        "# hyper params\n",
        "EPOCHS = [32, 16, 8, 4]\n",
        "BATCH_SIZE = 64\n",
        "initial_scale = 1.\n",
        "GPU = 0\n",
        "\n",
        "# extension params\n",
        "log_interval = 1000\n",
        "display_interval = None\n",
        "tweet_interval = 1000\n",
        "snapshot_interval = 1000\n",
        "\n",
        "# model params\n",
        "sa_gamma = 1.\n",
        "IMG_SIZE = 256\n",
        "CH_SIZE = 8\n",
        "LATENT_SIZE = 64\n",
        "HIDDEN_SIZE = 16\n",
        "\n",
        "# learning controller\n",
        "learning_rate = 2e-4\n",
        "grad_clip = None\n",
        "gen_weight_decay = 0\n",
        "dis_weight_decay = 0\n",
        "sa_endpoint = 15000\n",
        "\n",
        "# file names\n",
        "load_gen = None\n",
        "load_dis = None\n",
        "\n",
        "OUT = './result/'\n",
        "dataroot = './picture/train_pic/flickr/landscape'\n",
        "\n",
        "gen_name = '{}_gen'.format(target)\n",
        "dis_name = '{}_dis'.format(target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "taibgWsxewAT",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import uuid\n",
        "import pickle\n",
        "import tweepy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm, clip_grad_norm_\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import twitter_api_key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sMBmUGbfmmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(size):\n",
        "    return torch.normal(torch.zeros(size), torch.ones(size))\n",
        "\n",
        "def zeropad(x, ch):\n",
        "    return F.pad(x, (0, 0, 0, 0, 0, ch-x.size(1), 0, 0))\n",
        "\n",
        "def split_zeropad(x, ch):\n",
        "    h = x.view(x.size(0), 4, -1, *x.size()[2:])\n",
        "    h_size_half = h.size(2) // 2\n",
        "    h_pad_half = ch // 8 - h_size_half\n",
        "    h = torch.cat(\n",
        "        (\n",
        "            F.pad(h[:,:,:h_size_half,:,:], (0,0,0,0,0,h_pad_half,0,0,0,0)).view(h.size(0), -1, *h.size()[3:]),\n",
        "            F.pad(h[:,:,h_size_half:,:,:], (0,0,0,0,h_pad_half,0,0,0,0,0)).view(h.size(0), -1, *h.size()[3:])\n",
        "         ),\n",
        "        dim=1\n",
        "    )\n",
        "    return h\n",
        "\n",
        "def gap(x):\n",
        "    return F.avg_pool2d(x, x.size()[-2:])\n",
        "\n",
        "def instance_var(x):\n",
        "    _shape = x.size()\n",
        "    _x = x.view(_shape[0], _shape[1], -1)\n",
        "    _x = torch.var(_x, dim=2)\n",
        "    _x = _x.view(*_x.size(), 1, 1)\n",
        "    _x = _x.expand(*_shape)\n",
        "    return _x\n",
        "\n",
        "def pixel_norm(x):\n",
        "    return x * torch.rsqrt((x**2).mean(1, keepdim=True) + 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wieMKd6RFRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Mish, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Softplus(),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x*self.main(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "255V-NaDeLIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=1, stride=1, padding=0, bias=True):\n",
        "        super(SNConv, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            Mish(),\n",
        "            spectral_norm(nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=bias))\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWziEjG0ZooZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNRes(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1):\n",
        "        super(SNRes, self).__init__()\n",
        "        self.main = SNConv(in_ch, out_ch, kernel, stride, padding)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdvHzeEdlxFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNDense(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(SNDense, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            Mish(),\n",
        "            spectral_norm(nn.Linear(in_ch, out_ch))\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.main(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz5PrYJQex1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNSelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, gamma=1.):\n",
        "        super(SNSelfAttentionBlock, self).__init__()\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.cf = SNConv(in_ch, out_ch//8)\n",
        "        self.cg = SNConv(in_ch, out_ch//8)\n",
        "        self.ch = SNConv(in_ch, out_ch)\n",
        "        self.softmax = nn.Softmax(2)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        f = self.cf(x)\n",
        "        g = self.cg(x)\n",
        "        h = self.ch(x)\n",
        "        f = f.view(f.size(0), f.size(1), -1)\n",
        "        g = g.view(g.size(0), g.size(1), -1)\n",
        "        h = h.view(h.size(0), h.size(1), -1)\n",
        "        \n",
        "        attention_map = torch.bmm(torch.transpose(f, 1, 2), g)\n",
        "        attention_map = self.softmax(attention_map)\n",
        "        feature_map = torch.bmm(h, torch.transpose(attention_map, 1, 2))\n",
        "        feature_map = feature_map.view(*x.size())\n",
        "\n",
        "        return x + feature_map*self.gamma\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        self.gamma = gamma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yv0ZlRisUb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MinibatchDiscrimination(nn.Module):\n",
        "    def __init__(self, in_ch, kernel, kernel_dims, device):\n",
        "        super(MinibatchDiscrimination, self).__init__()\n",
        "        self.device = device\n",
        "        self.kernel = kernel\n",
        "        self.dim = kernel_dims\n",
        "        self.t = nn.Linear(in_ch, self.kernel*self.dim, bias=False)\n",
        "        for param in self.t.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def __call__(self, x):\n",
        "        batchsize = x.size(0)\n",
        "        m = self.t(x).view(batchsize, self.kernel, self.dim, 1)\n",
        "        m_T = torch.transpose(m, 0, 3)\n",
        "        m, m_T = torch.broadcast_tensors(m, m_T)\n",
        "        norm = torch.sum(F.l1_loss(m, m_T, reduction='none'), dim=2)\n",
        "\n",
        "        eraser = torch.eye(batchsize, device=self.device).view(batchsize, 1, batchsize).expand(norm.size())\n",
        "        c_b = torch.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = torch.sum(c_b, dim=2)\n",
        "        h = torch.cat((x, o_b), dim=1)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By7GhkDdFG_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(InceptionBlock, self).__init__()\n",
        "\n",
        "        self.layer_11 = SNConv(in_ch, mid_ch)\n",
        "        self.layer_33_1 = nn.Sequential(\n",
        "            SNConv(in_ch, mid_ch),\n",
        "            SNConv(mid_ch, mid_ch, 3, 1, 1)\n",
        "        )\n",
        "        self.layer_33_2 = nn.Sequential(\n",
        "            SNConv(in_ch, mid_ch),\n",
        "            SNConv(mid_ch, mid_ch, 3, 1, 1),\n",
        "            SNRes(mid_ch, mid_ch)\n",
        "        )\n",
        "\n",
        "        self.c = SNConv(mid_ch*3, out_ch)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        h_11 = self.layer_11(x)\n",
        "        h_33_1 = self.layer_33_1(x)\n",
        "        h_33_2 = self.layer_33_2(x)\n",
        "        \n",
        "        h = torch.cat((h_11, h_33_1, h_33_2), dim=1)\n",
        "        h = self.c(h)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLAucx40aUr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InceptionResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(InceptionResBlock, self).__init__()\n",
        "        self.ch = out_ch\n",
        "        self.inception = InceptionBlock(in_ch, self.ch//4, self.ch)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        _h = self.inception(x)\n",
        "        h = zeropad(x, self.ch)\n",
        "        return h + _h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-drx6j6OvQpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, sa_gamma=None, downconv=True):\n",
        "        super(DiscriminatorBlock, self).__init__()\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "        self.extractor = SNConv(in_ch, out_ch, 3, 1, 1) if not downconv else None\n",
        "        self.sa = SNSelfAttentionBlock(out_ch, out_ch, gamma=sa_gamma) if sa_gamma else None\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(2)\n",
        "        self.downconv = SNConv(in_ch  , out_ch, 4, 2, 1) if downconv else None\n",
        "            \n",
        "    def forward(self, x):\n",
        "        h = self.extractor(x) if self.extractor is not None else x\n",
        "\n",
        "        if self.sa:\n",
        "            h = self.sa(h)\n",
        "\n",
        "        if self.downconv:\n",
        "            _h = zeropad(self.downsample(x), self.out_ch)\n",
        "            h = self.downconv(h) + _h\n",
        "\n",
        "        return h\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        if self.sa:\n",
        "            self.sa.set_gamma(gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, out_ch=2, device=None, sa_gamma=1.):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            spectral_norm(nn.Conv2d(3, 16, 1, 1, 0)),\n",
        "            DiscriminatorBlock(16, 32),\n",
        "            DiscriminatorBlock(32, 64),\n",
        "            DiscriminatorBlock(64, 128),\n",
        "            DiscriminatorBlock(128, 256),\n",
        "            DiscriminatorBlock(256, 512),\n",
        "            DiscriminatorBlock(512, 512),\n",
        "            DiscriminatorBlock(512, 512, sa_gamma=sa_gamma, downconv=False),\n",
        "            nn.AvgPool2d(4),\n",
        "            nn.Flatten(),\n",
        "            MinibatchDiscrimination(512, 64, 16, device),\n",
        "            SNDense(512+64, out_ch)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        self.main[6].set_gamma(gamma)\n",
        "        self.main[7].set_gamma(gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT6qSzQ-hq_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentGenerator(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, device=None):\n",
        "        super(LatentGenerator, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.mu = spectral_norm(nn.Linear(in_ch, out_ch))\n",
        "        self.var = spectral_norm(nn.Linear(in_ch, out_ch))\n",
        "\n",
        "    def forward(self, w):\n",
        "        mu = self.mu(w)\n",
        "        var = self.var(w)\n",
        "        h = mu + var * gaussian(var.size()).to(self.device)\n",
        "        return pixel_norm(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scShQFu8ttNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, device=None):\n",
        "        super(NoiseInjection, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.var = spectral_norm(nn.Linear(in_ch, out_ch))\n",
        "\n",
        "    def forward(self, x, w):\n",
        "        var = self.var(w)\n",
        "        var = var.view(*var.size(), 1, 1).expand(*x.size())\n",
        "        noise = gaussian(x.size()).to(self.device)\n",
        "        return x + noise * var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zTVuBAFqAjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DepthwiseCondConv(nn.Module):\n",
        "    def __init__(self, in_ch, n_kernels, kernel=3, stride=1, padding=1, device=None, k=1):\n",
        "        super(DepthwiseCondConv, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.ch = in_ch\n",
        "        self.kernels_size = (n_kernels, kernel, kernel)\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty(n_kernels*kernel*kernel, k, 1))\n",
        "        nn.init.normal_(self.weight)\n",
        "        self.activation = Mish()\n",
        "    \n",
        "    def forward(self, x, sigma, bias=None, activation=True):\n",
        "        b_size = x.size(0)\n",
        "\n",
        "        h = self.activation(x) if activation else x\n",
        "        \n",
        "        kernels = self.weight.expand(-1, -1, self.ch)\n",
        "\n",
        "        i = torch.eye(self.ch).to(self.device)\n",
        "        i = i.view(1, self.ch, self.ch)\n",
        "        i = i.expand(kernels.size(0), -1, -1)\n",
        "        \n",
        "        kernels = i*kernels\n",
        "        kernels = kernels.view(*self.kernels_size, self.ch, -1)\n",
        "        kernels = kernels.transpose(1, 3).transpose(2, 4)\n",
        "        kernels = kernels.expand(b_size, -1, -1, -1, -1, -1)\n",
        "\n",
        "        _s = sigma.view(*kernels.size()[:3], 1, 1, 1)\n",
        "        _s = _s.expand(-1, -1, -1, self.ch, -1, -1)\n",
        "        _s = torch.sum(kernels*_s, dim=1)\n",
        "        _s = _s.reshape(b_size, self.ch, -1)\n",
        "        _s = F.normalize(_s, dim=2)\n",
        "\n",
        "        h = F.unfold(h, self.kernel, padding=self.padding, stride=self.stride)\n",
        "        h = torch.bmm(_s, h)\n",
        "        h = h.view(b_size, self.ch, *x.size()[2:])\n",
        "\n",
        "        if bias is not None:\n",
        "            _b = bias.view(-1, self.ch, 1, 1)\n",
        "            h = h + _b\n",
        "        \n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yiNF-rUsXo9i",
        "colab": {}
      },
      "source": [
        "class SynthBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, latent_ch, mid_ch, kernels, device=None):\n",
        "        super(SynthBlock, self).__init__()\n",
        "\n",
        "        self.lg = LatentGenerator(latent_ch, mid_ch, device)\n",
        "        self.sigma = spectral_norm(nn.Linear(mid_ch, in_ch*kernels))\n",
        "\n",
        "        self.noise_injection = NoiseInjection(latent_ch, in_ch, device)\n",
        "        self.dcc = spectral_norm(DepthwiseCondConv(in_ch, kernels, device=device))\n",
        "        self.c = SNConv(in_ch, out_ch, bias=False)\n",
        "            \n",
        "    def forward(self, x, w, dcc_activation=True):\n",
        "        _w = self.lg(w)\n",
        "        sigma = self.sigma(_w)\n",
        "\n",
        "        h = self.noise_injection(x, w)\n",
        "        h = self.dcc(h, sigma, activation=dcc_activation)\n",
        "        h = self.c(h)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNjgKfTYQEMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OutputGenerator(nn.Module):\n",
        "    def __init__(self, ch_size, bias=False):\n",
        "        super(OutputGenerator, self).__init__()\n",
        "        self.ch_size = ch_size\n",
        "\n",
        "        self.c_out = SNConv(ch_size, 3, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.ch_size != x.size(1):\n",
        "            h = x.view(x.size(0), self.ch_size//2, -1, *x.size()[2:])\n",
        "            h = torch.cat((h[:,:,0,:,:], h[:,:,-1,:,:]), dim=1)\n",
        "        else:\n",
        "            h = x\n",
        "\n",
        "        h = self.c_out(h)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afBTb9IzXPxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, latent_ch, mid_ch, kernels, ch_size,\n",
        "                 device=None, sa_gamma=None, bias=False):\n",
        "        super(GeneratorBlock, self).__init__()\n",
        "        self.synth_0 = SynthBlock(in_ch, out_ch*4, latent_ch, mid_ch, kernels, device)\n",
        "        self.synth_1 = SynthBlock(out_ch*4, out_ch*4, latent_ch, mid_ch*4, kernels, device)\n",
        "        self.sa = SNSelfAttentionBlock(out_ch*4, out_ch*4, gamma=sa_gamma) if sa_gamma else None\n",
        "        self.out = OutputGenerator(ch_size, bias)\n",
        "            \n",
        "    def forward(self, x, w, input_activation=False):\n",
        "        h = self.synth_0(x, w, input_activation)\n",
        "        h = h + self.synth_1(h, w)\n",
        "\n",
        "        if self.sa:\n",
        "            h = self.sa(h)\n",
        "\n",
        "        return self.out(h), h\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        if self.sa:\n",
        "            self.sa.set_gamma(gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C6ziVvO3vV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LastGeneratorBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, latent_ch, mid_ch, kernels, ch_size, device=None):\n",
        "        super(LastGeneratorBlock, self).__init__()\n",
        "        self.synth_0 = SynthBlock(in_ch, out_ch, latent_ch, mid_ch, kernels, device)\n",
        "        self.synth_1 = SynthBlock(out_ch, out_ch, latent_ch, mid_ch, kernels, device)\n",
        "            \n",
        "    def forward(self, x, w, input_activation=False):\n",
        "        h = self.synth_0(x, w, input_activation)\n",
        "        h = h + self.synth_1(h, w)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, ch_size, latent_size, hidden_size, device=None, sa_gamma=1., kernels=6):\n",
        "        super(Generator, self).__init__()\n",
        "        self.device = device\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.upbi = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.upps = nn.PixelShuffle(2)\n",
        "\n",
        "        self.btm = nn.Parameter(torch.empty(1, ch_size*32, 4, 4))\n",
        "        nn.init.normal_(self.btm)\n",
        "        \n",
        "        self.res6 = GeneratorBlock(ch_size*32, ch_size*16, latent_size, hidden_size*16, kernels, ch_size,\n",
        "                                   device, sa_gamma, bias=True)\n",
        "        self.res5 = GeneratorBlock(ch_size*16, ch_size*16, latent_size, hidden_size*16, kernels, ch_size,\n",
        "                                   device)\n",
        "        self.res4 = GeneratorBlock(ch_size*16, ch_size*8, latent_size, hidden_size*8, kernels, ch_size, device)\n",
        "        self.res3 = GeneratorBlock(ch_size*8, ch_size*4, latent_size, hidden_size*4, kernels, ch_size, device)\n",
        "        self.res2 = GeneratorBlock(ch_size*4, ch_size*2, latent_size, hidden_size*2, kernels, ch_size, device)\n",
        "        self.res1 = GeneratorBlock(ch_size*2, ch_size, latent_size, hidden_size, kernels, ch_size, device)\n",
        "        self.res0 = LastGeneratorBlock(ch_size, ch_size, latent_size, hidden_size, kernels, ch_size, device)\n",
        "        \n",
        "        self.out = SNConv(ch_size, 3)\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        w = pixel_norm(gaussian((batch, self.latent_size)).to(self.device))\n",
        "        h = self.btm.expand(batch, -1, -1, -1)\n",
        "        o, h = self.res6(h, w, input_activation=False)\n",
        "\n",
        "        _o, h = self.res5(self.upps(h), w)\n",
        "        o = _o + self.upbi(o)\n",
        "\n",
        "        _o, h = self.res4(self.upps(h), w)\n",
        "        o = _o + self.upbi(o)\n",
        "\n",
        "        _o, h = self.res3(self.upps(h), w)\n",
        "        o = _o + self.upbi(o)\n",
        "\n",
        "        _o, h = self.res2(self.upps(h), w)\n",
        "        o = _o + self.upbi(o)\n",
        "\n",
        "        _o, h = self.res1(self.upps(h), w)\n",
        "        o = _o + self.upbi(o)\n",
        "\n",
        "        h = self.res0(self.upps(h), w)\n",
        "        h = self.out(h)\n",
        "        o = h + self.upbi(o)\n",
        "\n",
        "        return torch.tanh(o)\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        self.res6.set_gamma(gamma)\n",
        "        self.res5.set_gamma(gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RvlFlUWVgCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:{}\".format(GPU))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjzxAjvT6VPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    if type(m) in (nn.Linear, nn.Conv2d):\n",
        "        nn.init.xavier_normal_(m.weight, gain=initial_scale)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif type(m) in (nn.BatchNorm1d, nn.BatchNorm2d):\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOCsFkxT8yAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(CH_SIZE, LATENT_SIZE, HIDDEN_SIZE, device=device)\n",
        "if load_gen:\n",
        "    gen.load_state_dict(torch.load(OUT+'{}_0.4.0_{}epoch.pkl'.format(gen_name, load_gen)))\n",
        "    gen.eval()\n",
        "else:\n",
        "    gen.apply(weights_init)\n",
        "\n",
        "dis = Discriminator(out_ch=1, device=device)\n",
        "if load_dis:\n",
        "    dis.load_state_dict(torch.load(OUT+'{}_0.4.0_{}epoch.pkl'.format(dis_name, load_dis)))\n",
        "    dis.eval()\n",
        "else:\n",
        "    dis.apply(weights_init)\n",
        "\n",
        "gen.to(device)\n",
        "dis.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh2tOT6nbjxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = optim.Adam(\n",
        "    gen.parameters(),\n",
        "    lr=learning_rate,\n",
        "    betas=(0.5, 0.999),\n",
        "    weight_decay=gen_weight_decay\n",
        "    )\n",
        "opt_dis = optim.Adam(\n",
        "    dis.parameters(),\n",
        "    lr=learning_rate,\n",
        "    betas=(0.5, 0.999),\n",
        "    weight_decay=dis_weight_decay\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWIg15KG17lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_losses = list()\n",
        "dis_losses = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qBbx84zdKHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tweet_interval:\n",
        "    auth = tweepy.OAuthHandler(twitter_api_key.CONSUMER_KEY, twitter_api_key.CONSUMER_SECRET)\n",
        "    auth.set_access_token(twitter_api_key.ACCESS_TOKEN_KEY, twitter_api_key.ACCESS_TOKEN_SECRET)\n",
        "    api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPMnP12KDyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_log(i, epoch, g_loss, d_loss, g_mean, d_real_mean, d_fake_mean):\n",
        "    print('[{}/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\tD(x): {:.4f}\\tD(G(z)): {:.4f}/{:.4f}'.format(\n",
        "        i, epoch, d_loss, g_loss, d_real_mean, d_fake_mean, g_mean\n",
        "    ))\n",
        "    gen_losses.append(g_loss)\n",
        "    dis_losses.append(d_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_image(gen, img_size):\n",
        "    clear_output()\n",
        "    with torch.no_grad():\n",
        "        generated = gen(8).detach().cpu()\n",
        "    generated = np.transpose(np.reshape(generated, (-1, 3, img_size, img_size)), (0, 2, 3, 1))\n",
        "\n",
        "    plt.figure(figsize=(16, 8))\n",
        "        \n",
        "    for i, img in enumerate(generated):\n",
        "        plt.subplot(2, 4, i+1).axis('off')\n",
        "        plt.subplot(2, 4, i+1).imshow(Image.fromarray(np.uint8((img+1.)/2. *255.)))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLZoVZoMey7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_upload(image_array, api):\n",
        "    bin_io = io.BytesIO()\n",
        "    img = Image.fromarray(np.uint8((image_array+1.)/2. *255.))\n",
        "    img = img.resize((512, 512), resample=0)\n",
        "    img.save(bin_io, format='JPEG')\n",
        "    result = api.media_upload(filename='{}_generated_{}.jpg'.format(target, uuid.uuid4()), file=bin_io)\n",
        "    return result.media_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myBTJoApd-lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_image(gen, api, img_size, iteration, epoch):\n",
        "    with torch.no_grad():\n",
        "        generated = gen(16).detach().cpu()\n",
        "    generated = np.reshape(generated, (4, 2, 2, 3, img_size, img_size))\n",
        "    generated = np.transpose(generated, (0, 3, 1, 4, 2, 5))\n",
        "    generated = np.reshape(generated, (4, 3, img_size*2, img_size*2))\n",
        "    generated = np.transpose(generated, (0, 2, 3, 1))\n",
        "\n",
        "    try:\n",
        "        img_ids = [image_upload(img, api) for img in generated]\n",
        "        hash_tags = ['AIで{}を作る'.format(target_JP),\n",
        "                    'iteration/epoch: {}/{}'.format(iteration+1, epoch+1),\n",
        "                    '#makeing{}'.format(target),\n",
        "                    '#nowlearning...',\n",
        "                    '#AI',\n",
        "                    '#人工知能',\n",
        "                    '#DeepLearning',\n",
        "                    '#GAN']\n",
        "\n",
        "        api.update_status(\n",
        "            status='\\n'.join(hash_tags),\n",
        "            media_ids=img_ids\n",
        "            )\n",
        "    except Exception:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u84LrAZ6Fm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(gen, dis, epoch):\n",
        "    torch.save(gen.state_dict(), OUT+'{}_0.4.0_{}epoch.pkl'.format(gen_name, epoch))\n",
        "    torch.save(dis.state_dict(), OUT+'{}_0.4.0_{}epoch.pkl'.format(dis_name, epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyoRMEDz8DUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_result():\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(gen_losses,label=\"G\")\n",
        "    plt.plot(dis_losses,label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qg08QSICrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def round_dataset(gen, dis, opt_gen, opt_dis, dataloader, epoch, img_size, device):\n",
        "    loss_fun = nn.MSELoss()\n",
        "    data_len = len(dataloader)\n",
        "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
        "        dis.zero_grad()\n",
        "        x_real = data[0].to(device)\n",
        "        b_size = x_real.size(0)\n",
        "        y_real = dis(x_real).view(-1)\n",
        "        real_loss = loss_fun(y_real, torch.ones(*y_real.size(), device=device))\n",
        "        real_loss.backward()\n",
        "\n",
        "        x_fake = gen(b_size)\n",
        "        y_fake = dis(x_fake.detach()).view(-1)\n",
        "        fake_loss = loss_fun(y_fake, torch.zeros(*y_fake.size(), device=device))\n",
        "        fake_loss.backward()\n",
        "\n",
        "        if grad_clip:\n",
        "            clip_grad_norm_(dis.parameters(), grad_clip)\n",
        "        dis_loss = real_loss + fake_loss\n",
        "        opt_dis.step()\n",
        "        \n",
        "        gen.zero_grad()\n",
        "        y_gen = dis(x_fake).view(-1)\n",
        "        gen_loss = loss_fun(y_gen, torch.ones(*y_gen.size(), device=device))\n",
        "        gen_loss.backward()\n",
        "\n",
        "        if grad_clip:\n",
        "            clip_grad_norm_(gen.parameters(), grad_clip)\n",
        "        opt_gen.step()\n",
        "\n",
        "        if display_interval and (i+1) % display_interval == 0:\n",
        "            make_image(gen, img_size)\n",
        "\n",
        "        if (i+1) % log_interval == 0:\n",
        "            report_log(\n",
        "                i,\n",
        "                data_len,\n",
        "                gen_loss.item(),\n",
        "                dis_loss.item(),\n",
        "                y_gen.mean().item(),\n",
        "                y_real.mean().item(),\n",
        "                y_fake.mean().item()\n",
        "                )\n",
        "\n",
        "        if tweet_interval and (i+1) % tweet_interval == 0:\n",
        "            post_image(gen, api, img_size, i, epoch)\n",
        "        \n",
        "        if snapshot_interval and (i+1) % snapshot_interval == 0:\n",
        "            save_model(gen, dis, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR5sYeZJI9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop(gen, dis, opt_gen, opt_dis, dataloader, epoch, img_size, device):\n",
        "    for i in range(epoch):\n",
        "        gen.train()\n",
        "        dis.train()\n",
        "        round_dataset(gen, dis, opt_gen, opt_dis, dataloader, i, img_size, device)\n",
        "    \n",
        "    report_result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRdjtxJi4glT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GaussianBlur():\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if self.k != 0:\n",
        "            return img.filter(ImageFilter.GaussianBlur(self.k))\n",
        "        else:\n",
        "            return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in range(3, -1, -1):\n",
        "    dataset = dset.ImageFolder(root=dataroot,\n",
        "                                transform=transforms.Compose([\n",
        "                                                                transforms.Resize(IMG_SIZE),\n",
        "                                                                transforms.RandomCrop(IMG_SIZE),\n",
        "                                                                transforms.RandomHorizontalFlip(),\n",
        "                                                                GaussianBlur(k),\n",
        "                                                                transforms.ToTensor(),\n",
        "                                                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                                                ]))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    train_loop(gen, dis, opt_gen, opt_dis, dataloader, EPOCHS[k], IMG_SIZE, device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqzasKfBlk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_image(gen, IMG_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}