{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GauGANnanteHEdemonai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "btB-TcgGrzsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iteration = 300\n",
        "batchsize = 16\n",
        "snapshot_interval = 20\n",
        "display_interval = 20\n",
        "update_interval = 20\n",
        "phase = 0\n",
        "'''\n",
        "0- : all decoder training.\n",
        "1- : trans encoding.(hold)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnDuRC0v0Ru",
        "colab_type": "code",
        "outputId": "afc5123c-3086-4f02-a4a8-70d9687cebd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mount = '/content/gdrive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(mount)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGFtpuE3Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import math\n",
        "import glob\n",
        "from PIL import Image, ImageOps, ImageChops, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy\n",
        "\n",
        "import chainer\n",
        "from chainer import training, backend, Variable\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import chainer.backends.cuda\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGloZh9xEu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(size):\n",
        "    return F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                     cupy.ones(size, dtype=cupy.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9P2_zkbFjeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer_Adam(model, alpha=1e-4, beta1=0.9, clip=True):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n",
        "    optimizer.setup(model)\n",
        "    if clip:\n",
        "        optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(1.))\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def out_generated_image(gen, c_iter, v_iter):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        clear_output()\n",
        "        xp = gen.xp\n",
        "        c_base = xp.asarray(c_iter.next())\n",
        "        z_c = Variable(c_base[:,1])/255. *2. -1.\n",
        "        if phase < 7:\n",
        "            v_base = c_base[:,0]\n",
        "        else:\n",
        "            v_base = xp.asarray(v_iter.next())[:,0]\n",
        "        z_v = Variable(v_base)/255. *2. -1.\n",
        "        \n",
        "        with chainer.using_config('train', False):\n",
        "            x, _ = gen(z_c, z_v)\n",
        "        x = F.transpose(F.reshape(x, (-1, 3, 128, 128)), (0, 2, 3, 1))\n",
        "        x = chainer.backends.cuda.to_cpu(x.array)\n",
        "        \n",
        "        plt.figure(figsize=(16, 12))\n",
        "        \n",
        "        \n",
        "        for i, img in enumerate(xp.asnumpy(c_base[:,1])):\n",
        "            plt.subplot(3,4,i+1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "            \n",
        "        for i, img in enumerate(x):\n",
        "            plt.subplot(3,4,i+5)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "            \n",
        "        for i, img in enumerate(xp.asnumpy(v_base)):\n",
        "            plt.subplot(3,4,i+9)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "        \n",
        "        plt.show()\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorGen(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(DiscriminatorGen, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.c0_0 = L.Convolution2D(3, 32, 3, 1, 1, initialW=w)\n",
        "            self.bn0_0 = L.BatchNormalization(32)\n",
        "            self.c0_1 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.bn0_1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c1_0 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.bn1_0 = L.BatchNormalization(64)\n",
        "            self.c1_1 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.bn1_1 = L.BatchNormalization(64)\n",
        "            self.c1_2 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.bn1_2 = L.BatchNormalization(64)\n",
        "            self.c1_3 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn1_3 = L.BatchNormalization(64)\n",
        "            self.c1_4 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn1_4 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c2_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn2_0 = L.BatchNormalization(128)\n",
        "            self.c2_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn2_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c3_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn3_0 = L.BatchNormalization(256)\n",
        "            self.c3_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn3_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.c4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn4_0 = L.BatchNormalization(512)\n",
        "            self.c4_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn4_1= L.BatchNormalization(512)\n",
        "            \n",
        "            self.c5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn5_0 = L.BatchNormalization(512)\n",
        "            self.c5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.l_m = L.Linear(None, 1024, initialW=w)\n",
        "            self.t = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        batchsize = m.shape[0]\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def gaussian(self, x):\n",
        "        return F.gaussian(x, cupy.ones(x.shape, dtype=cupy.float32)*2e-2)\n",
        "    \n",
        "    def self_attention(self, x, f, g, h, gamma=0.01):\n",
        "        f_shape = f.shape\n",
        "        g_shape = g.shape\n",
        "        h_shape = h.shape\n",
        "        \n",
        "        attention_map = f*F.transpose(g, (0, 1, 3, 2))\n",
        "        feature_map = h*attention_map\n",
        "        return F.add(x, feature_map*gamma)\n",
        "    \n",
        "    def gap(self, x):\n",
        "        return F.average_pooling_2d(x, x.shape[3])\n",
        "    \n",
        "    def forward(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        x = F.leaky_relu(self.bn0_0(self.c0_0(self.gaussian(z))))\n",
        "        _x = F.leaky_relu(self.bn0_1(self.c0_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 128\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        f = F.tanh(self.bn1_0(self.c1_0(x)))\n",
        "        g = F.tanh(self.bn1_1(self.c1_1(x)))\n",
        "        h = F.tanh(self.bn1_2(self.c1_2(x)))\n",
        "        x = F.leaky_relu(self.self_attention(x, f, g, h))\n",
        "        x = F.leaky_relu(self.bn1_3(self.c1_3(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn1_4(self.c1_4(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 64\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn2_0(self.c2_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn2_1(self.c2_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 32\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn3_0(self.c3_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn3_1(self.c3_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 16\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn4_0(self.c4_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn4_1(self.c4_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 8\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn5_0(self.c5_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn5_1(self.c5_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        x = self.gap(x)\n",
        "        # 4\n",
        "        \n",
        "        h_m = F.leaky_relu(self.l_m(x))\n",
        "        m = F.reshape(self.t(h_m), (batchsize, 64, 16))\n",
        "        o_b = self.minibatch_discrimination(m)\n",
        "        \n",
        "        x = F.concat((h_m, o_b), axis=1)\n",
        "        \n",
        "        return self.l_o(x), h_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXTfbl5A3P7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorEn(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(DiscriminatorEn, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.l0 = L.Linear(None, 1024, initialW=w)\n",
        "            self.l1 = L.Linear(None, 1024, initialW=w)\n",
        "            self.t = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        batchsize = m.shape[0]\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def forward(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        h = F.relu(self.l0(z))\n",
        "        h = F.relu(self.l1(h))\n",
        "        m = F.reshape(self.t(h), (batchsize, 64, 16))\n",
        "        o_b = self.minibatch_discrimination(m)\n",
        "        \n",
        "        out = F.concat((h, o_b), axis=1)\n",
        "        \n",
        "        return self.l_o(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.stc0_0 = L.Convolution2D(3, 32, 3, 1, 1, initialW=w)\n",
        "            self.stbn0_0 = L.BatchNormalization(32)\n",
        "            self.stc0_1 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.stbn0_1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.stc1_0 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.stbn1_0 = L.BatchNormalization(64)\n",
        "            self.stc1_1 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.stbn1_1 = L.BatchNormalization(64)\n",
        "            self.stc1_2 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.stbn1_2 = L.BatchNormalization(64)\n",
        "            self.stc1_3 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.stbn1_3 = L.BatchNormalization(64)\n",
        "            self.stc1_4 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.stbn1_4 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.stc2_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.stbn2_0 = L.BatchNormalization(128)\n",
        "            self.stc2_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.stbn2_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.stc3_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.stbn3_0 = L.BatchNormalization(256)\n",
        "            self.stc3_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.stbn3_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.stc4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.stbn4_0 = L.BatchNormalization(512)\n",
        "            self.stc4_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.stbn4_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.stc5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.stbn5_0 = L.BatchNormalization(512)\n",
        "            self.stc5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.stbn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.stl1 = L.Linear(None, 1024, initialW=w)\n",
        "            self.stl2 = L.Linear(None, 1024, initialW=w)\n",
        "            self.stl3 = L.Linear(None, 1024, initialW=w)\n",
        "            \n",
        "            self.stbtm = L.Linear(None, 512*8*8, initialW=w)\n",
        "            \n",
        "            self.enc0_0 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc0_0 = L.BatchNormalization(32)\n",
        "            self.enc0_1 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.enbnc0_1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.enc1_0 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.enbnc1_0 = L.BatchNormalization(128)\n",
        "            self.enc1_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.enbnc1_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.enc2_0 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.enbnc2_0 = L.BatchNormalization(128)\n",
        "            self.enc2_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.enbnc2_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.enc3_0 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.enbnc3_0 = L.BatchNormalization(128)\n",
        "            self.enc3_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.enbnc3_1 = L.BatchNormalization(128)\n",
        "\n",
        "            self.enc_gamma = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.enbnc_gamma = L.BatchNormalization(128)\n",
        "            self.enc_beta = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.enbnc_beta = L.BatchNormalization(128)\n",
        "            \n",
        "            self.de5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn5_0 = L.BatchNormalization(512)\n",
        "            self.de5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.de4_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.debn4_0 = L.BatchNormalization(256)\n",
        "            self.de4_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.debn4_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.de3_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.debn3_0 = L.BatchNormalization(128)\n",
        "            self.de3_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.debn3_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.de2_0 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.debn2_0 = L.BatchNormalization(64)\n",
        "            self.de2_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.debn2_1 = L.BatchNormalization(64)\n",
        "            self.de2_2 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.debn2_2 = L.BatchNormalization(128)\n",
        "            self.de2_3 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.debn2_3 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.de1_0 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.debn1_0 = L.BatchNormalization(32)\n",
        "            self.de1_1 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.debn1_1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c_out = L.Convolution2D(None, 3, 1, 1, 0, initialW=w)\n",
        "            \n",
        "    def self_attention(self, x, f, g, h, gamma=0.01):\n",
        "        f_shape = f.shape\n",
        "        g_shape = g.shape\n",
        "        h_shape = h.shape\n",
        "        \n",
        "        attention_map = f*F.transpose(g, (0, 1, 3, 2))\n",
        "        feature_map = h*attention_map\n",
        "        return F.add(x, feature_map*gamma)\n",
        "    \n",
        "    def gap(self, x):\n",
        "        return F.average_pooling_2d(x, x.shape[3])\n",
        "    \n",
        "    def spade(self, feature, gamma, beta):\n",
        "        x = feature*gamma\n",
        "        x = F.add(x, beta)\n",
        "        return x\n",
        "    \n",
        "    def gaussian(self, batch, ch, w, h):\n",
        "        size = batch*ch*w*h\n",
        "        noise = F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                           cupy.ones(size, dtype=cupy.float32))\n",
        "        return F.reshape(noise, (batch, ch, w, h))\n",
        "    \n",
        "    def forward(self, x, style):        \n",
        "        batch = x.shape[0]\n",
        "        # style\n",
        "        z = F.relu(self.stbn0_0(self.stc0_0(style)))\n",
        "        _z = F.relu(self.stbn0_1(self.stc0_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 128\n",
        "        z = F.max_pooling_2d(z, 2)\n",
        "        f = F.tanh(self.stbn1_0(self.stc1_0(z)))\n",
        "        g = F.tanh(self.stbn1_1(self.stc1_1(z)))\n",
        "        h = F.tanh(self.stbn1_2(self.stc1_2(z)))\n",
        "        z = F.relu(self.self_attention(z, f, g, h))\n",
        "        z = F.relu(self.stbn1_3(self.stc1_3(z)))\n",
        "        _z = F.relu(self.stbn1_4(self.stc1_4(z)))\n",
        "        z = F.concat((z,_z))\n",
        "        # 64\n",
        "        z = F.max_pooling_2d(z, 2)\n",
        "        z = F.relu(self.stbn2_0(self.stc2_0(z)))\n",
        "        _z = F.relu(self.stbn2_1(self.stc2_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 32\n",
        "        z = F.max_pooling_2d(z, 2)\n",
        "        z = F.relu(self.stbn3_0(self.stc3_0(z)))\n",
        "        _z = F.relu(self.stbn3_1(self.stc3_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 16\n",
        "        z = F.max_pooling_2d(z, 2)\n",
        "        z = F.relu(self.stbn4_0(self.stc4_0(z)))\n",
        "        _z = F.relu(self.stbn4_1(self.stc4_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 8\n",
        "        z = F.max_pooling_2d(z, 2)\n",
        "        z = F.relu(self.stbn5_0(self.stc5_0(z)))\n",
        "        _z = F.relu(self.stbn5_1(self.stc5_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 4\n",
        "        z = self.gap(z)\n",
        "        \n",
        "        z = F.relu(self.stl1(z))\n",
        "        z = F.relu(self.stl2(z))\n",
        "        z = F.relu(self.stl3(z))\n",
        "        \n",
        "        # Encode\n",
        "        s = F.relu(self.enbnc0_0(self.enc0_0(x)))\n",
        "        f = F.relu(self.enbnc0_1(self.enc0_1(x)))\n",
        "        # 128\n",
        "        s_64 = F.space2depth(s, 2)\n",
        "        f = F.space2depth(f, 2)\n",
        "        s_64 = F.relu(self.enbnc1_0(self.enc1_0(s_64)))\n",
        "        f = F.relu(self.enbnc1_1(self.enc1_1(f)))\n",
        "        # 64\n",
        "        s_32 = F.average_pooling_2d(s_64, 2)\n",
        "        f = F.max_pooling_2d(f, 2)\n",
        "        s_32 = F.relu(self.enbnc2_0(self.enc2_0(s_32)))\n",
        "        f = F.relu(self.enbnc2_1(self.enc2_1(f)))\n",
        "        # 32\n",
        "        s = F.average_pooling_2d(s_32, 2)\n",
        "        f = F.max_pooling_2d(f, 2)\n",
        "        s = F.relu(self.enbnc3_0(self.enc3_0(s)))\n",
        "        f = F.relu(self.enbnc3_1(self.enc3_1(f)))\n",
        "        f = F.relu(f*s)\n",
        "        # 16\n",
        "        f = F.unpooling_2d(f, 2, cover_all=False)\n",
        "        f = F.relu(f*s_32)\n",
        "        # 32\n",
        "        f = F.unpooling_2d(f, 2, cover_all=False)\n",
        "        f = F.relu(f*s_64)\n",
        "        g = F.relu(self.enbnc_gamma(self.enc_gamma(f)))\n",
        "        b = F.relu(self.enbnc_beta(self.enc_beta(f)))\n",
        "        # 64\n",
        "        \n",
        "        # Decode\n",
        "        h = F.relu(F.reshape(self.stbtm(z), (-1, 512, 8, 8)))\n",
        "        n = self.gaussian(batch, 1, 8, 8)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.relu(self.debn5_0(self.de5_0(h)))\n",
        "        n = self.gaussian(batch, 1, 8, 8)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.relu(self.debn5_1(self.de5_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        # 8\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian(batch, 1, 16, 16)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.relu(self.debn4_0(self.de4_0(h)))\n",
        "        n = self.gaussian(batch, 1, 16, 16)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.relu(self.debn4_1(self.de4_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        # 16\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian(batch, 1, 32, 32)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.relu(self.debn3_0(self.de3_0(h)))\n",
        "        n = self.gaussian(batch, 1, 32, 32)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.relu(self.debn3_1(self.de3_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        # 32\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian(batch, 1, 64, 64)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.relu(self.debn2_0(self.de2_0(h)))\n",
        "        n = self.gaussian(batch, 1, 64, 64)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.relu(self.debn2_1(self.de2_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        h = F.relu(self.debn2_2(self.de2_2(h)))\n",
        "        h = F.relu(self.spade(h, g, b))\n",
        "        h = F.relu(self.debn2_3(self.de2_3(h)))\n",
        "        # 64\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian(batch, 1, 128, 128)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.relu(self.debn1_0(self.de1_0(h)))\n",
        "        n = self.gaussian(batch, 1, 128, 128)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.relu(self.debn1_1(self.de1_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        \n",
        "        h = F.tanh(self.c_out(h))\n",
        "        # 128\n",
        "        \n",
        "        return h, z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkPjj5gHE76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis_gen, self.dis_en = kwargs.pop('models')\n",
        "        self.cache_fake = None\n",
        "        self.cache_dis_gen = None\n",
        "        self.cache_dis_en = None\n",
        "        self.k = 5\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def loss_dis(self, dis, ll, opt):\n",
        "        loss = sum(ll)\n",
        "        chainer.report({opt: loss}, dis)\n",
        "        return loss\n",
        "    \n",
        "    def loss_en(self, en, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'style_rec': ll[-1]}, en)\n",
        "        return loss\n",
        "    \n",
        "    def loss_gen(self, gen, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'fake': ll[0], 'rec': ll[-1]}, gen)\n",
        "        return loss\n",
        "\n",
        "    def update_core(self):\n",
        "        gen_optimizer = self.get_optimizer('gen')\n",
        "        dis_gen_optimizer = self.get_optimizer('dis_gen')\n",
        "        dis_en_optimizer = self.get_optimizer('dis_en')\n",
        "        \n",
        "        gen, dis_gen, dis_en = self.gen, self.dis_gen, self.dis_en\n",
        "        main_iter = self.get_iterator('main')\n",
        "        style_iter = self.get_iterator('style')\n",
        "        \n",
        "        for _ in range(self.k):\n",
        "            batch_c = main_iter.next()\n",
        "            batchsize = len(batch_c)\n",
        "            x_c = Variable(self.converter(batch_c, device=self.device)) /255. *2. -1.\n",
        "            x_c_real = x_c[:,0]\n",
        "            x_c_base = x_c[:,1]\n",
        "\n",
        "            batch_v = style_iter.next()\n",
        "            x_v_real = Variable(self.converter(batch_v, device=self.device))[:,0] /255. *2. -1.\n",
        "            \n",
        "            if phase >= 1:\n",
        "                x_fake, h_v_real = gen(x_c_base, x_v_real)\n",
        "            else:\n",
        "                x_fake, h_v_real = gen(x_c_base, x_c_real)\n",
        "            \n",
        "            y_c_real, fm_c_real = dis_gen(x_c_real)\n",
        "            dis_loss_real = F.sum((y_c_real-1)**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_real], 'c_real')\n",
        "\n",
        "            y_v_real, fm_v_real = dis_gen(x_v_real)\n",
        "            dis_loss_real = F.sum((y_v_real-1)**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_real], 'v_real')\n",
        "            \n",
        "            y_fake, fm_fake = dis_gen(x_fake)\n",
        "            dis_loss_fake = F.sum(y_fake**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_fake], 'fake')\n",
        "        \n",
        "            if self.cache_fake is not None:\n",
        "                remind, _ = dis_gen(self.cache_fake)\n",
        "                dis_loss_remind = F.sum(remind**2) / batchsize\n",
        "                dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_remind], 'remind')\n",
        "\n",
        "            self.cache_fake = x_fake\n",
        "                \n",
        "            if self.cache_dis_gen is None:\n",
        "                self.cache_dis_gen = dis_gen.copy('copy')\n",
        "                \n",
        "        dis_gen.copyparams(self.cache_dis_gen)\n",
        "        self.cache_dis_gen = None\n",
        "        \n",
        "        fake_loss = F.sum((y_fake-1)**2) / batchsize\n",
        "            \n",
        "        if phase < 1:\n",
        "            fm_loss = F.mean_squared_error(fm_c_real, fm_fake)\n",
        "            reconst_loss = F.mean_squared_error(x_c_real, x_fake)\n",
        "        else:\n",
        "            fm_loss = F.mean_squared_error(fm_v_real, fm_fake)\n",
        "            reconst_loss = F.mean_squared_error(x_c_base, x_fake)\n",
        "\n",
        "        gen_optimizer.update(self.loss_gen, gen, [fake_loss, fm_loss, reconst_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifY8to6Hhsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = 0\n",
        "OUT = mount+'/My Drive/result/'\n",
        "IMG_SHAPE = (128, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6bmkEnHoQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = mount+'/My Drive/picture/train_pic/**/*.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49zXHF0QHq6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator()\n",
        "dis_gen = DiscriminatorGen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6m896tHwsp",
        "colab_type": "code",
        "outputId": "d56b41a7-3190-4f7e-e13d-b042178b1b2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chainer.backends.cuda.get_device_from_id(GPU).use()\n",
        "gen.to_gpu()\n",
        "dis_gen.to_gpu()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DiscriminatorEn at 0x7f92cb7cf780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBvJVx3H8xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = make_optimizer_Adam(gen, clip=False)\n",
        "opt_dis_gen = make_optimizer_Adam(dis_gen, clip=False)\n",
        "dis_gen.t.disable_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3yTVR2tGV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sq(img):\n",
        "    img_width, img_height = img.size\n",
        "    sub = img_width - img_height\n",
        "    if sub == 0:\n",
        "        return img\n",
        "    crop_size = min(img.size)\n",
        "    crop_position = numpy.random.randint(0, abs(sub))\n",
        "    if sub > 0:\n",
        "        return img.crop((crop_position, 0, crop_size+crop_position, crop_size))\n",
        "    else:\n",
        "        return img.crop((0, crop_position, crop_size, crop_size+crop_position))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr27raJaIDGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_convert(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    width, height = img.size\n",
        "    image_trans_trigger = numpy.random.randn(2)\n",
        "    \n",
        "    img = crop_sq(img)\n",
        "        \n",
        "    if image_trans_trigger[0] > 0:\n",
        "        img = ImageOps.mirror(img)\n",
        "        \n",
        "    if image_trans_trigger[1] > 0:\n",
        "        img = img.resize(IMG_SHAPE)\n",
        "    else:\n",
        "        img = img.resize((IMG_SHAPE[0]+16, IMG_SHAPE[1]+16))\n",
        "        crop_position = numpy.random.randint(0, 16, 2)\n",
        "        img = img.crop((crop_position[0],\n",
        "                       crop_position[1],\n",
        "                       crop_position[0]+IMG_SHAPE[0],\n",
        "                       crop_position[1]+IMG_SHAPE[1]))\n",
        "\n",
        "    real_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    img = img.filter(ImageFilter.GaussianBlur(1.0))\n",
        "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
        "    img = ImageOps.equalize(img)\n",
        "    img = img.quantize(8, kmeans=True).convert('RGB')\n",
        "    \n",
        "    base_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    out_array = numpy.concatenate((real_array[None, :, :, :],\n",
        "                                   base_array[None, :, :, :]), axis=0)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1l6Zuw9IHKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_resize(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    img = crop_sq(img)\n",
        "      \n",
        "    img = img.resize(IMG_SHAPE)\n",
        "    \n",
        "    real_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    img = img.filter(ImageFilter.GaussianBlur(1.0))\n",
        "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
        "    img = ImageOps.equalize(img)\n",
        "    img = img.quantize(8, kmeans=True).convert('RGB')\n",
        "    \n",
        "    base_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    out_array = numpy.concatenate((real_array[None, :, :, :],\n",
        "                                   base_array[None, :, :, :]), axis=0)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xynzSKG4IKYP",
        "colab_type": "code",
        "outputId": "302c098e-8be8-46b6-ad6a-5d96d171a89d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "image_files = glob.glob(dataset, recursive=True)\n",
        "print('{} contains {} image files'\n",
        "      .format(dataset, len(image_files)))\n",
        "img_dataset = chainer.datasets.ImageDataset(paths=image_files)\n",
        "\n",
        "train = chainer.datasets.TransformDataset(img_dataset, img_convert)\n",
        "sample = chainer.datasets.TransformDataset(img_dataset, img_resize)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/picture/train_pic/**/*.jpg contains 1062026 image files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsS_Vf0KIOQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_c_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)\n",
        "train_v_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToPAI31hIQoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "    models=(gen, dis_gen),\n",
        "    iterator={'main': train_v_iter, 'style': train_c_iter},\n",
        "    optimizer={'gen': opt_gen, 'dis_gen': opt_dis_gen},\n",
        "    device=GPU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5phMAnIWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = training.Trainer(updater, (iteration, 'iteration'), out=OUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqcOBLPUIaSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o2TkGb1Icnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(out_generated_image(gen,\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True),\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True)),\n",
        "               trigger=snapshot_interval)\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'dis_gen/c_real', 'dis_gen/fake', 'gen/loss'\n",
        "    ]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=update_interval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNdaVJ4gy9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if phase > 0:\n",
        "    chainer.serializers.load_npz(OUT+'korosu_gen.npz', gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'korosu_dis_gen.npz', dis_gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'korosu_opt_gen.npz', opt_gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'korosu_opt_dis_gen.npz', opt_dis_gen, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epn0ZUHekKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chainer.serializers.save_npz(OUT+'korosu_gen.npz', gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_dis_gen.npz', dis_gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_opt_gen.npz', opt_gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_opt_dis_gen.npz', opt_dis_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}