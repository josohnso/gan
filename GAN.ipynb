{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_LANDSCAPE_pretrain_Generator_v0.9.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VERSION = '0.9.2'\n",
        "GPU = 0\n",
        "\n",
        "# target\n",
        "TARGET = 'landscape'\n",
        "TARGET_JP = '風景'\n",
        "\n",
        "# hyper params\n",
        "EPOCHS = (8, 8, 8, 16, 16, 32, 32)\n",
        "BATCH_SIZES = (256, 256, 256, 256, 128, 128, 64)\n",
        "\n",
        "# extension params\n",
        "LOG_INTERVAL = 2000\n",
        "DISPLAY_INTERVAL = None\n",
        "TWEET_INTERVAL = 8000\n",
        "SNAPSHOT_INTERVAL = 2000\n",
        "\n",
        "# model params\n",
        "SA_GAMMA = 1.\n",
        "CH_SIZE = 32\n",
        "LATENT_SIZE = 256\n",
        "STAGE = 7\n",
        "\n",
        "# learning params\n",
        "LEARNING_RATE = 2e-4\n",
        "GRAD_CLIP = None\n",
        "GEN_WEIGHT_DECAY = 0\n",
        "DIS_WEIGHT_DECAY = 0\n",
        "RUNNING_MEAN_RATE = 1e-4\n",
        "RECONST_RATE = 1.\n",
        "START = 0\n",
        "\n",
        "# initialize params\n",
        "LOAD_GEN = None\n",
        "LOAD_DIS = None\n",
        "STRICT = False\n",
        "INIT_LAYER = (4, -1)\n",
        "\n",
        "# paths\n",
        "OUT = './result/'\n",
        "DATAROOT = './picture/train_pic/flickr/landscape'\n",
        "\n",
        "# names\n",
        "GEN_NAME = '{}_gen'.format(TARGET)\n",
        "DIS_NAME = '{}_dis'.format(TARGET)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "taibgWsxewAT",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import random\n",
        "import uuid\n",
        "import pickle\n",
        "import tweepy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm, clip_grad_norm_\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import twitter_api_key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWHr5FjWs5SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = torch.device(\"cuda:{}\".format(GPU))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sMBmUGbfmmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zeropad(x, ch):\n",
        "    return F.pad(x, (0, 0, 0, 0, 0, ch-x.size(1), 0, 0))\n",
        "\n",
        "def gap(x):\n",
        "    return F.avg_pool2d(x, x.size()[-2:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88W1Yb7u5lj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=1, stride=1, padding=0, bias=True, groups=1, activation=nn.LeakyReLU(0.2)):\n",
        "        super(SNConv, self).__init__()\n",
        "        self.conv = spectral_norm(nn.Conv2d(in_ch, out_ch, kernel, stride, padding, bias=bias, groups=groups))\n",
        "\n",
        "        self.activation = activation\n",
        "\n",
        "        self._cache_weight = None\n",
        "        self._cache_bias = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.activation(x) if self.activation else x\n",
        "        return self.conv(h)\n",
        "\n",
        "    def weight_init(self):\n",
        "        nn.init.kaiming_normal_(self.conv.weight, 0.2)\n",
        "        if self.conv.bias is not None:\n",
        "            nn.init.constant_(self.conv.bias.data, 0)\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        if self.conv.weight.requires_grad:\n",
        "            if self._cache_weight is None:\n",
        "                self._cache_weight = self.conv.weight.data.detach()\n",
        "            else:\n",
        "                self._cache_weight = torch.lerp(self.conv.weight.data, self._cache_weight, gamma)\n",
        "                self.conv.weight.data = self._cache_weight\n",
        "        else:\n",
        "            if self._cache_weight is not None:\n",
        "                self._cache_weight = None\n",
        "\n",
        "        if self.conv.bias is not None and self.conv.bias.requires_grad:\n",
        "            if self._cache_bias is None:\n",
        "                self._cache_bias = self.conv.bias.data.detach()\n",
        "            else:\n",
        "                self._cache_bias = torch.lerp(self.conv.bias.data, self._cache_bias, gamma)\n",
        "                self.conv.bias.data = self._cache_bias\n",
        "        else:\n",
        "            if self._cache_bias is not None:\n",
        "                self._cache_bias = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-VcNdgQ5uWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNDense(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bias=True, activation=nn.LeakyReLU(0.2)):\n",
        "        super(SNDense, self).__init__()\n",
        "        self.linear = spectral_norm(nn.Linear(in_ch, out_ch, bias=bias))\n",
        "        \n",
        "        self.activation = activation\n",
        "\n",
        "        self._cache_weight = None\n",
        "        self._cache_bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.activation(x) if self.activation else x\n",
        "        return self.linear(h)\n",
        "\n",
        "    def weight_init(self):\n",
        "        nn.init.kaiming_normal_(self.linear.weight, 0.2)\n",
        "        if self.linear.bias is not None:\n",
        "            nn.init.constant_(self.linear.bias.data, 0)\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        if self.linear.weight.requires_grad:\n",
        "            if self._cache_weight is None:\n",
        "                self._cache_weight = self.linear.weight.data.detach()\n",
        "            else:\n",
        "                self._cache_weight = torch.lerp(self.linear.weight.data, self._cache_weight, gamma)\n",
        "                self.linear.weight.data = self._cache_weight\n",
        "        else:\n",
        "            if self._cache_weight is not None:\n",
        "                self._cache_weight = None\n",
        "\n",
        "        if self.linear.bias is not None and self.linear.bias.requires_grad:\n",
        "            if self._cache_bias is None:\n",
        "                self._cache_bias = self.linear.bias.data.detach()\n",
        "            else:\n",
        "                self._cache_bias = torch.lerp(self.linear.bias.data, self._cache_bias, gamma)\n",
        "                self.linear.bias.data = self._cache_bias\n",
        "        else:\n",
        "            if self._cache_bias is not None:\n",
        "                self._cache_bias = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz5PrYJQex1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNSelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, gamma=1.):\n",
        "        super(SNSelfAttentionBlock, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.cf = SNConv(in_ch, out_ch//8)\n",
        "        self.cg = SNConv(in_ch, out_ch//8)\n",
        "        self.ch = SNConv(in_ch, out_ch)\n",
        "\n",
        "        self.softmax = nn.Softmax(2)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        f = self.cf(x)\n",
        "        g = self.cg(x)\n",
        "        h = self.ch(x)\n",
        "        f = f.view(f.size(0), f.size(1), -1)\n",
        "        g = g.view(g.size(0), g.size(1), -1)\n",
        "        h = h.view(h.size(0), h.size(1), -1)\n",
        "        \n",
        "        attention_map = torch.bmm(torch.transpose(f, 1, 2), g)\n",
        "        attention_map = self.softmax(attention_map)\n",
        "        feature_map = torch.bmm(h, torch.transpose(attention_map, 1, 2))\n",
        "        feature_map = feature_map.view(*x.size())\n",
        "\n",
        "        return x+feature_map*self.gamma\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def weight_init(self):\n",
        "        self.cf.weight_init()\n",
        "        self.cg.weight_init()\n",
        "        self.ch.weight_init()\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        self.cf.running_mean(gamma)\n",
        "        self.cg.running_mean(gamma)\n",
        "        self.ch.running_mean(gamma)\n",
        "\n",
        "    def freeze_param(self):\n",
        "        self.cf.requires_grad_(False)\n",
        "        self.cg.requires_grad_(False)\n",
        "        self.ch.requires_grad_(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yv0ZlRisUb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MinibatchDiscrimination(nn.Module):\n",
        "    def __init__(self, in_ch, kernel, kernel_dims, device):\n",
        "        super(MinibatchDiscrimination, self).__init__()\n",
        "        self.device = device\n",
        "        self.kernel = kernel\n",
        "        self.dim = kernel_dims\n",
        "        \n",
        "        self.t = SNDense(in_ch, self.kernel*self.dim, bias=False, activation=False)\n",
        "        for param in self.t.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def __call__(self, x):\n",
        "        batchsize = x.size(0)\n",
        "        m = self.t(x).view(batchsize, self.kernel, self.dim, 1)\n",
        "        m_T = torch.transpose(m, 0, 3)\n",
        "        m, m_T = torch.broadcast_tensors(m, m_T)\n",
        "        norm = torch.sum(F.l1_loss(m, m_T, reduction='none'), dim=2)\n",
        "\n",
        "        eraser = torch.eye(batchsize, device=self.device).view(batchsize, 1, batchsize).expand(norm.size())\n",
        "        c_b = torch.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = torch.sum(c_b, dim=2)\n",
        "        h = torch.cat((x, o_b), dim=1)\n",
        "        return h\n",
        "\n",
        "    def weight_init(self):\n",
        "        self.t.weight_init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-drx6j6OvQpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, sa_gamma=None, downconv=True):\n",
        "        super(DiscriminatorBlock, self).__init__()\n",
        "        self.out_ch = out_ch\n",
        "\n",
        "        self.sa = SNSelfAttentionBlock(out_ch, out_ch, gamma=sa_gamma) if sa_gamma else None\n",
        "        self.extractor = SNConv(in_ch, out_ch, 4, 2, 1) if downconv else SNConv(in_ch, out_ch, 3, 1, 1)\n",
        "        \n",
        "        self.downsample = nn.AvgPool2d(2) if downconv else None\n",
        "            \n",
        "    def forward(self, x):\n",
        "        h = x if self.sa is None else self.sa(x)\n",
        "        h = self.extractor(h)\n",
        "\n",
        "        _h = x if self.downsample is None else self.downsample(x)\n",
        "        _h = zeropad(_h, self.out_ch)\n",
        "        return h + _h\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        if self.sa:\n",
        "            self.sa.set_gamma(gamma)\n",
        "\n",
        "    def weight_init(self):\n",
        "        if self.sa is not None:\n",
        "            self.sa.weight_init()\n",
        "        self.extractor.weight_init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ch_size, out_ch=1, device=None, sa_gamma=1.):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            SNConv(3, ch_size, 1, 1, 0, activation=False),\n",
        "            DiscriminatorBlock(ch_size, ch_size),\n",
        "            DiscriminatorBlock(ch_size, ch_size),\n",
        "            DiscriminatorBlock(ch_size, ch_size, downconv=False),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            MinibatchDiscrimination(ch_size, 8, 8, device),\n",
        "            SNDense(ch_size+8, out_ch)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.main(x)\n",
        "        return h\n",
        "\n",
        "    def set_gamma(self, gamma):\n",
        "        return\n",
        "        \n",
        "    def weight_init(self):\n",
        "        for m in self.main[:4]:\n",
        "            m.weight_init()\n",
        "        for m in self.main[6:]:\n",
        "            m.weight_init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxmjUXGNPjvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CondConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, hidden_ch, kernel=1, padding=0, variations=3,\n",
        "                 activation=nn.LeakyReLU(0.2), bias=True):\n",
        "        super(CondConv, self).__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.out_ch = out_ch\n",
        "        self.variations = variations\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty(out_ch, in_ch, variations, kernel**2))\n",
        "        self.bias = nn.Parameter(torch.empty(out_ch)) if bias else None\n",
        "\n",
        "        self.condition = SNDense(hidden_ch, out_ch*in_ch*variations, activation=None)\n",
        "\n",
        "        self.activation = activation\n",
        "        self.unfold = nn.Unfold(kernel, padding=padding)\n",
        "        self.softmax = nn.Softmax(3)\n",
        "\n",
        "        self._cache_weight = None\n",
        "        self._cache_bias = None\n",
        "    \n",
        "    def forward(self, x, z):\n",
        "        b_size, _, height, width = x.size()\n",
        "        h = self.activation(x) if self.activation else x\n",
        "\n",
        "        w = self.weight.expand(b_size, -1, -1, -1, -1)\n",
        "\n",
        "        f = self.condition(z)\n",
        "        f = f.view(b_size, self.out_ch, self.in_ch, self.variations)\n",
        "        f = self.softmax(f)\n",
        "        f = f.view(*f.size(), 1)\n",
        "\n",
        "        w = w * f\n",
        "        w = w.sum(dim=3)\n",
        "        w = w.view(b_size, self.out_ch, -1)\n",
        "\n",
        "        h = self.unfold(h)\n",
        "        h = torch.bmm(w, h)\n",
        "        h = h.view(b_size, self.out_ch, height, width)\n",
        "        if self.bias is not None:\n",
        "            _b = self.bias.view(1, -1, 1, 1)\n",
        "            _b = _b.expand(b_size, -1, height, width)\n",
        "            h = h + _b\n",
        "        return h\n",
        "\n",
        "    def weight_init(self):\n",
        "        nn.init.kaiming_normal_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.constant_(self.bias.data, 0)\n",
        "        self.condition.weight_init()\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        if self.weight.requires_grad:\n",
        "            if self._cache_weight is None:\n",
        "                self._cache_weight = self.weight.data.detach()\n",
        "            else:\n",
        "                self._cache_weight = torch.lerp(self.weight.data, self._cache_weight, gamma)\n",
        "                self.weight.data = self._cache_weight\n",
        "        else:\n",
        "            if self._cache_weight is not None:\n",
        "                self._cache_weight = None\n",
        "\n",
        "        if self.bias is not None and self.bias.requires_grad:\n",
        "            if self._cache_bias is None:\n",
        "                self._cache_bias = self.bias.data.detach()\n",
        "            else:\n",
        "                self._cache_bias = torch.lerp(self.bias.data, self._cache_bias, gamma)\n",
        "                self.bias.data = self._cache_bias\n",
        "        else:\n",
        "            if self._cache_bias is not None:\n",
        "                self._cache_bias = None\n",
        "\n",
        "        self.condition.running_mean(gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9m30ObSTeOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GeneratorBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, latent_ch, activation=nn.LeakyReLU(0.2), device=None):\n",
        "        super(GeneratorBlock, self).__init__()\n",
        "        self.out_ch = out_ch\n",
        "        self.latent_ch = latent_ch\n",
        "        self.device = device\n",
        "\n",
        "        self.extractor_0 = spectral_norm(CondConv(in_ch, out_ch, latent_ch, kernel=3, padding=1,\n",
        "                                                  activation=activation))\n",
        "        self.extractor_1 = spectral_norm(CondConv(out_ch, out_ch, latent_ch, kernel=3, padding=1))\n",
        "        self.tail = spectral_norm(CondConv(out_ch, out_ch, latent_ch, kernel=3, padding=1))\n",
        "\n",
        "        self._cache_tail = None\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        h = self.extractor_0(x, z)\n",
        "        h = self.extractor_1(h, z)\n",
        "\n",
        "        _h = None if self._cache_tail is None else self._cache_tail(h, z)\n",
        "        h = self.tail(h, z)\n",
        "        return h, _h\n",
        "\n",
        "    def weight_init(self):\n",
        "        self.extractor_0.weight_init()\n",
        "        self.extractor_1.weight_init()\n",
        "        self.tail.weight_init()\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        self.extractor_0.running_mean(gamma)\n",
        "        self.extractor_1.running_mean(gamma)\n",
        "        self.tail.running_mean(gamma)\n",
        "\n",
        "    def freeze_param(self, freeze_tail=False):\n",
        "        self.extractor_0.requires_grad_(False)\n",
        "        self.extractor_1.requires_grad_(False)\n",
        "        if freeze_tail:\n",
        "            self.tail.requires_grad_(False)\n",
        "\n",
        "    def cache_tail(self):\n",
        "        self._cache_tail = spectral_norm(\n",
        "            CondConv(self.out_ch, self.out_ch, self.latent_ch, kernel=3, padding=1)\n",
        "            )\n",
        "        self._cache_tail.load_state_dict(self.tail.state_dict())\n",
        "        self._cache_tail.to(self.device)\n",
        "        self._cache_tail.requires_grad_(False)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        self._cache_tail = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ch_size, latent_size, device=None):\n",
        "        super(Generator, self).__init__()\n",
        "        self.device = device\n",
        "        self.ch_size = ch_size\n",
        "        self.latent_size = latent_size\n",
        "\n",
        "        self.btm = SNDense(latent_size, ch_size*4, activation=None)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device),\n",
        "             GeneratorBlock(ch_size//4+1, ch_size, latent_size, device=device)]\n",
        "             )\n",
        "        self.out = SNConv(ch_size, 3)\n",
        "\n",
        "        self.upbi = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.upps = nn.PixelShuffle(2)\n",
        "\n",
        "        self._cache_out = None\n",
        "\n",
        "    def forward(self, batch, manipulation=None, layer_num=6):\n",
        "        if manipulation is not None:\n",
        "            z = manipulation\n",
        "        else:\n",
        "            z = self.latents_generate(batch)\n",
        "\n",
        "        _h = None\n",
        "        h = self.btm(z)\n",
        "        h = h.view(batch, self.ch_size//4, 4, 4)\n",
        "        _n = torch.randn(batch, 1, *h.size()[2:], device=self.device)\n",
        "        h = torch.cat((h, _n), dim=1)\n",
        "        \n",
        "        for block in self.blocks[:layer_num]:\n",
        "            h, _h = self.block_up(h, z, block)\n",
        "            _n = torch.randn(batch, 1, *h.size()[2:], device=self.device)\n",
        "            h = torch.cat((h, _n), dim=1)\n",
        "\n",
        "        if self._cache_out is not None:\n",
        "            _h = self._cache_out(_h)\n",
        "            _h = torch.tanh(_h)\n",
        "\n",
        "        h, _ = self.blocks[layer_num](h, z)\n",
        "        h = self.out(h)\n",
        "        return torch.tanh(h), _h\n",
        "\n",
        "    def block_up(self, x, z, block):\n",
        "        h, _h = block(x, z)\n",
        "        h = self.upps(h)\n",
        "        return h, _h\n",
        "\n",
        "    def latents_generate(self, batch):\n",
        "        return self.pixel_norm(torch.randn(batch, self.latent_size, device=self.device))\n",
        "\n",
        "    def pixel_norm(self, x):\n",
        "        return x * torch.rsqrt((x**2).mean(1, keepdim=True) + 1e-8)\n",
        "\n",
        "    def weight_init(self, init_layer):\n",
        "        if init_layer[0] == 0:\n",
        "            self.btm.weight_init()\n",
        "        for b in self.blocks[init_layer[0]:init_layer[1]]:\n",
        "            b.weight_init()\n",
        "\n",
        "        nn.init.xavier_normal_(self.out.conv.weight)\n",
        "        if self.out.conv.bias is not None:\n",
        "            nn.init.constant_(self.out.conv.bias.data, 0)\n",
        "\n",
        "    def running_mean(self, gamma):\n",
        "        self.btm.running_mean(gamma)\n",
        "        for b in self.blocks:\n",
        "            b.running_mean(gamma)\n",
        "        self.out.running_mean(gamma)\n",
        "\n",
        "    def up_scale(self, layer_num):\n",
        "        if layer_num == 0:\n",
        "            self.btm.requires_grad_(False)\n",
        "\n",
        "        self.blocks[layer_num-1].freeze_param(freeze_tail=True)\n",
        "        self.blocks[layer_num-1].clear_cache()\n",
        "\n",
        "        self.blocks[layer_num].freeze_param()\n",
        "        self.blocks[layer_num].cache_tail()\n",
        "        \n",
        "        self._cache_out = SNConv(self.ch_size, 3)\n",
        "        self._cache_out.load_state_dict(self.out.state_dict())\n",
        "        self._cache_out.to(self.device)\n",
        "        self._cache_out.requires_grad_(False)\n",
        "\n",
        "    def clear_cache(self):\n",
        "        self._cache_out = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCqH0zBObDqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pixel_norm(x):\n",
        "    return x * torch.rsqrt((x**2).mean(1, keepdim=True) + 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KVnj5T1QWjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONST_LATENT = pixel_norm(torch.randn(16, LATENT_SIZE, device=DEVICE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMYJcmlY4jb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_discriminator(ch_size=CH_SIZE, device=DEVICE, sa_gamma=SA_GAMMA):\n",
        "    dis = Discriminator(ch_size, out_ch=1, device=device, sa_gamma=sa_gamma)\n",
        "    if LOAD_DIS:\n",
        "        dis.load_state_dict(torch.load(OUT+'{}_{}_stage{}.pkl'.format(DIS_NAME, VERSION, LOAD_DIS)))\n",
        "        dis.eval()\n",
        "    else:\n",
        "        dis.weight_init()\n",
        "    dis.to(device)\n",
        "\n",
        "    opt_dis = optim.Adam(\n",
        "        dis.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "        weight_decay=DIS_WEIGHT_DECAY\n",
        "        )\n",
        "    return dis, opt_dis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOCsFkxT8yAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(CH_SIZE, LATENT_SIZE, device=DEVICE)\n",
        "if LOAD_GEN:\n",
        "    gen.load_state_dict(torch.load(OUT+'{}_{}_stage{}.pkl'.format(GEN_NAME, VERSION, LOAD_GEN)), strict=STRICT)\n",
        "    gen.weight_init(INIT_LAYER)\n",
        "    gen.eval()\n",
        "else:\n",
        "    gen.weight_init()\n",
        "gen.to(DEVICE)\n",
        "\n",
        "opt_gen = optim.Adam(\n",
        "    gen.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=(0.5, 0.999),\n",
        "    weight_decay=GEN_WEIGHT_DECAY\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWIg15KG17lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_losses = list()\n",
        "dis_losses = list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qBbx84zdKHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if TWEET_INTERVAL:\n",
        "    auth = tweepy.OAuthHandler(twitter_api_key.CONSUMER_KEY, twitter_api_key.CONSUMER_SECRET)\n",
        "    auth.set_access_token(twitter_api_key.ACCESS_TOKEN_KEY, twitter_api_key.ACCESS_TOKEN_SECRET)\n",
        "    api = tweepy.API(auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPMnP12KDyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_log(iteration, epoch, g_loss, d_loss, rec_loss, g_mean, d_real_mean, d_fake_mean):\n",
        "    print('[{}/{}]\\tLoss_D: {:.4f}\\tLoss_G: {:.4f}\\tLoss_Grec: {:.4f}\\tD(x): {:.4f}\\tD(G(z)): {:.4f}/{:.4f}'.format(\n",
        "        iteration, epoch, d_loss, g_loss, rec_loss, d_real_mean, d_fake_mean, g_mean\n",
        "    ))\n",
        "    gen_losses.append(g_loss)\n",
        "    dis_losses.append(d_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_image(gen, stage):\n",
        "    img_size = 2**(stage+2)\n",
        "\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        generated, _ = gen(16, manipulation=CONST_LATENT, layer_num=stage)\n",
        "        generated = generated.detach().cpu()\n",
        "    gen.train()\n",
        "\n",
        "    generated = np.transpose(np.reshape(generated, (-1, 3, img_size, img_size)), (0, 2, 3, 1))\n",
        "\n",
        "    plt.figure(figsize=(16, 16))  \n",
        "    for i, img in enumerate(generated):\n",
        "        plt.subplot(4, 4, i+1).axis('off')\n",
        "        plt.subplot(4, 4, i+1).imshow(Image.fromarray(np.uint8((img+1.)/2. *255.)))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLZoVZoMey7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_upload(image_array, api):\n",
        "    bin_io = io.BytesIO()\n",
        "    img = Image.fromarray(np.uint8((image_array+1.)/2. *255.))\n",
        "    img = img.resize((512, 512), resample=0)\n",
        "    img.save(bin_io, format='JPEG')\n",
        "    result = api.media_upload(filename='{}_generated_{}.jpg'.format(TARGET, uuid.uuid4()), file=bin_io)\n",
        "    return result.media_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myBTJoApd-lE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def post_image(gen, api, stage, iteration, epoch):\n",
        "    img_size = 2**(stage+2)\n",
        "\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        generated, _ = gen(16, manipulation=CONST_LATENT, layer_num=stage)\n",
        "        generated = generated.detach().cpu()\n",
        "    gen.train()\n",
        "\n",
        "    generated = np.reshape(generated, (4, 2, 2, 3, img_size, img_size))\n",
        "    generated = np.transpose(generated, (0, 3, 1, 4, 2, 5))\n",
        "    generated = np.reshape(generated, (4, 3, img_size*2, img_size*2))\n",
        "    generated = np.transpose(generated, (0, 2, 3, 1))\n",
        "\n",
        "    try:\n",
        "        img_ids = [image_upload(img, api) for img in generated]\n",
        "        hash_tags = ['AIで{}を作る'.format(TARGET_JP),\n",
        "                    'iteration/epoch: {}/{}'.format(iteration+1, epoch+1),\n",
        "                    '#makeing{}'.format(TARGET),\n",
        "                    '#nowlearning...',\n",
        "                    '#AI',\n",
        "                    '#人工知能',\n",
        "                    '#DeepLearning',\n",
        "                    '#GAN']\n",
        "\n",
        "        api.update_status(\n",
        "            status='\\n'.join(hash_tags),\n",
        "            media_ids=img_ids\n",
        "            )\n",
        "    except Exception:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u84LrAZ6Fm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(gen, dis, epoch):\n",
        "    torch.save(gen.state_dict(), OUT+'{}_{}_stage{}.pkl'.format(GEN_NAME, VERSION, epoch))\n",
        "    torch.save(dis.state_dict(), OUT+'{}_{}_stage{}.pkl'.format(DIS_NAME, VERSION, epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyoRMEDz8DUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_result():\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(gen_losses,label=\"G\")\n",
        "    plt.plot(dis_losses,label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMdHTbguATH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_discriminator(dis, opt_dis, real, fake, device=DEVICE):\n",
        "    loss_fun = nn.MSELoss()\n",
        "    \n",
        "    dis.zero_grad()\n",
        "    y_real = dis(real).view(-1)\n",
        "    real_loss = loss_fun(y_real, torch.ones(*y_real.size(), device=device))\n",
        "    real_loss.backward()\n",
        "\n",
        "    y_fake = dis(fake).view(-1)\n",
        "    fake_loss = loss_fun(y_fake, torch.zeros(*y_fake.size(), device=device))\n",
        "    fake_loss.backward()\n",
        "\n",
        "    if GRAD_CLIP:\n",
        "        clip_grad_norm_(dis.parameters(), GRAD_CLIP)\n",
        "    dis_loss = real_loss + fake_loss\n",
        "    opt_dis.step()\n",
        "\n",
        "    return dis_loss, y_real, y_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPlSycp2Acq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_generator(gen, opt_gen, dis, fake, prestage, epoch, device=DEVICE):\n",
        "    loss_fun = nn.MSELoss()\n",
        "    reconst_rate = RECONST_RATE\n",
        "\n",
        "    gen.zero_grad()\n",
        "    y_gen = dis(fake).view(-1)\n",
        "    gen_loss = loss_fun(y_gen, torch.ones(*y_gen.size(), device=device))\n",
        "\n",
        "    if prestage is None:\n",
        "        gen_loss.backward()\n",
        "        reconst_loss = None\n",
        "    else:\n",
        "        gen_loss.backward(retain_graph=True)\n",
        "\n",
        "        reconst_loss = loss_fun(\n",
        "            F.avg_pool2d(fake, 2),\n",
        "            prestage.detach())\n",
        "        reconst_loss = reconst_loss * reconst_rate\n",
        "        reconst_loss.backward()\n",
        "\n",
        "    if GRAD_CLIP:\n",
        "        clip_grad_norm_(gen.parameters(), GRAD_CLIP)\n",
        "    opt_gen.step()\n",
        "\n",
        "    gen.running_mean(RUNNING_MEAN_RATE*epoch)\n",
        "\n",
        "    return gen_loss, y_gen, reconst_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qg08QSICrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def round_dataset(gen, dis, opt_gen, opt_dis, dataloader, epoch, stage, device=DEVICE):\n",
        "    data_len = len(dataloader)\n",
        "    img_size = 2**(stage+2)\n",
        "\n",
        "    if epoch > 0:\n",
        "        dis.set_gamma(SA_GAMMA)\n",
        "\n",
        "    for iteration, data in tqdm(enumerate(dataloader, 0)):\n",
        "        if epoch == 0:\n",
        "            dis.set_gamma(SA_GAMMA*iteration/data_len)\n",
        "\n",
        "        x_real = data[0].to(device)\n",
        "        b_size = x_real.size(0)\n",
        "        x_fake, x_prestage = gen(b_size, layer_num=stage)\n",
        "\n",
        "        dis_loss, y_real, y_fake = update_discriminator(dis, opt_dis, x_real, x_fake.detach(), device=device)\n",
        "        gen_loss, y_gen, reconst_loss = update_generator(gen, opt_gen, dis, x_fake, x_prestage, epoch, device=device)\n",
        "\n",
        "        if DISPLAY_INTERVAL and (iteration+1) % DISPLAY_INTERVAL == 0:\n",
        "            make_image(gen, stage)\n",
        "\n",
        "        if (iteration+1) % LOG_INTERVAL == 0:\n",
        "            report_log(\n",
        "                iteration,\n",
        "                epoch,\n",
        "                gen_loss.item(),\n",
        "                dis_loss.item(),\n",
        "                reconst_loss.item() if reconst_loss is not None else 0,\n",
        "                y_gen.mean().item(),\n",
        "                y_real.mean().item(),\n",
        "                y_fake.mean().item()\n",
        "                )\n",
        "\n",
        "        if TWEET_INTERVAL and (iteration+1) % TWEET_INTERVAL == 0:\n",
        "            post_image(gen, api, stage, iteration, epoch)\n",
        "        \n",
        "        if SNAPSHOT_INTERVAL and (iteration+1) % SNAPSHOT_INTERVAL == 0:\n",
        "            save_model(gen, dis, stage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifAbO9PkA0zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grow_layer(gen, opt_gen, dataloader, stage, device=DEVICE):\n",
        "    dis, opt_dis = create_discriminator()\n",
        "    dis.train()\n",
        "    for epoch in range(EPOCHS[stage]):\n",
        "        round_dataset(gen, dis, opt_gen, opt_dis, dataloader, epoch, stage, device=device)\n",
        "    gen.up_scale(stage)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec99cjN_Vp1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataloader(stage):\n",
        "    img_size = 2**(stage+2)\n",
        "    dataset = dset.ImageFolder(root=DATAROOT,\n",
        "                                transform=transforms.Compose([transforms.Resize(img_size),\n",
        "                                                              transforms.RandomCrop(img_size),\n",
        "                                                              transforms.RandomHorizontalFlip(),\n",
        "                                                              transforms.ToTensor(),\n",
        "                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                                              ]))\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZES[stage], shuffle=True, num_workers=6, drop_last=True)\n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR5sYeZJI9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_loop(gen, opt_gen, device=DEVICE):\n",
        "    gen.train()\n",
        "    for stage in range(START, STAGE):\n",
        "        dataloader = create_dataloader(stage)\n",
        "        grow_layer(gen, opt_gen, dataloader, stage, device=device)\n",
        "    report_result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loop(gen, opt_gen, device=DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}