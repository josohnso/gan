{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnDuRC0v0Ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mount = '/content/gdrive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(mount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGFtpuE3Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import math\n",
        "import glob\n",
        "from PIL import Image, ImageOps, ImageChops, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy\n",
        "\n",
        "import chainer\n",
        "from chainer import training, backend, Variable\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import chainer.backends.cuda\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGloZh9xEu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(size):\n",
        "    return F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                     cupy.ones(size, dtype=cupy.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9P2_zkbFjeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer_Adam(model, alpha=1e-4, beta1=0.9, clip=True):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n",
        "    optimizer.setup(model)\n",
        "    if clip:\n",
        "        optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(1.))\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def out_generated_image(gen, c_iter, v_iter):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        clear_output()\n",
        "        xp = gen.xp\n",
        "        c_base = c_iter.next()\n",
        "        z_c = Variable(xp.asarray(c_base))/255. *2. -1.\n",
        "        v_base = v_iter.next()\n",
        "        z_v = Variable(xp.asarray(v_base))/255. *2. -1.\n",
        "        \n",
        "        with chainer.using_config('train', False):\n",
        "            x = gen(z_c, z_v)\n",
        "        x = F.transpose(F.reshape(x, (-1, 3, 256, 256)), (0, 2, 3, 1))\n",
        "        x = chainer.backends.cuda.to_cpu(x.array)\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        \n",
        "        for i, img in enumerate(c_base):\n",
        "            plt.subplot(3,4,i+1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "            \n",
        "        for i, img in enumerate(x):\n",
        "            plt.subplot(3,4,i+5)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "            \n",
        "        for i, img in enumerate(v_base):\n",
        "            plt.subplot(3,4,i+9)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "        \n",
        "        plt.show()\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqwTwrtIHhe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDeep(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(EncoderDeep, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            w_tanh = chainer.initializers.HeNormal(0.1)\n",
        "            \n",
        "            self.c0 = L.Convolution2D(3, 32, 5, 1, 2, initialW=w, nobias=True)\n",
        "            self.bn0 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c1 = L.Convolution2D(None, 32, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c2 = L.Convolution2D(None, 64, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn2 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c3 = L.Convolution2D(None, 128, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn3 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c4 = L.Convolution2D(None, 256, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn4 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.c5 = L.Convolution2D(None, 512, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn5 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c6 = L.Convolution2D(None, 1024, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn6 = L.BatchNormalization(1024)\n",
        "            \n",
        "            self.c7 = L.Convolution2D(None, 2048, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn7 = L.BatchNormalization(2048)\n",
        "            \n",
        "            self.l_mu = L.Linear(None, 1024, initialW=w)\n",
        "            self.l_var = L.Linear(None, 1024, initialW=w)\n",
        "            \n",
        "    def forward(self, z):\n",
        "        h = F.leaky_relu(self.bn0(self.c0(z)))\n",
        "        # 256\n",
        "        h = F.leaky_relu(self.bn1(self.c1(h)))\n",
        "        # 128\n",
        "        h = F.leaky_relu(self.bn2(self.c2(h)))\n",
        "        # 64\n",
        "        h = F.leaky_relu(self.bn3(self.c3(h)))\n",
        "        # 32\n",
        "        h = F.leaky_relu(self.bn4(self.c4(h)))\n",
        "        # 16\n",
        "        h = F.leaky_relu(self.bn5(self.c5(h)))\n",
        "        # 8\n",
        "        h = F.leaky_relu(self.bn6(self.c6(h)))\n",
        "        # 4\n",
        "        h = F.leaky_relu(self.bn7(self.c7(h)))\n",
        "        # 2\n",
        "        \n",
        "        mu = F.tanh(self.l_mu(h))\n",
        "        var = F.tanh(self.l_var(h))\n",
        "        \n",
        "        return mu, var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYAW64atFmub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderMiddle(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(EncoderMiddle, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            w_tanh = chainer.initializers.HeNormal(0.1)\n",
        "            \n",
        "            self.c0 = L.Convolution2D(3, 32, 5, 1, 2, initialW=w, nobias=True)\n",
        "            self.bn0 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c1 = L.Convolution2D(None, 32, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c2 = L.Convolution2D(None, 64, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn2 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c3 = L.Convolution2D(None, 128, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn3 = L.BatchNormalization(128)\n",
        "            self.co_32 = L.Convolution2D(None, 8, 1, 1, 0, initialW=w_tanh, nobias=True)\n",
        "            \n",
        "            self.c4 = L.Convolution2D(None, 256, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn4 = L.BatchNormalization(256)\n",
        "            self.co_16 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w_tanh, nobias=True)\n",
        "            \n",
        "            self.l_out = L.Linear(None, 1024, initialW=w)\n",
        "            \n",
        "    def forward(self, z):\n",
        "        batch = z.shape[0]\n",
        "        h = F.leaky_relu(self.bn0(self.c0(z)))\n",
        "        # 256\n",
        "        h = F.leaky_relu(self.bn1(self.c1(h)))\n",
        "        # 128\n",
        "        h = F.leaky_relu(self.bn2(self.c2(h)))\n",
        "        # 64\n",
        "        \n",
        "        h = F.leaky_relu(self.bn3(self.c3(h)))\n",
        "        h_32 = F.reshape(F.leaky_relu(self.co_32(h)), (batch, -1))\n",
        "        # 32\n",
        "        \n",
        "        h = F.leaky_relu(self.bn4(self.c4(h)))\n",
        "        h = F.reshape(F.leaky_relu(self.co_16(h)), (batch, -1))\n",
        "        h = F.concat((h, h_32))\n",
        "        # 16\n",
        "        \n",
        "        out = F.tanh(self.l_out(h))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlriprNc3a6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AffineTransfer(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(AffineTransfer, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            self.l0 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l1 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l2 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l3 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l4 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l5 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            self.l6 = L.Linear(None, 1024, initialW=w, nobias=True)\n",
        "            \n",
        "            self.l_out = L.Linear(None, 1024, initialW=w)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = F.relu(self.l0(z))\n",
        "        h = F.relu(self.l1(h))\n",
        "        h = F.relu(self.l2(h))\n",
        "        h = F.relu(self.l3(h))\n",
        "        h = F.relu(self.l4(h))\n",
        "        h = F.relu(self.l5(h))\n",
        "        h = F.relu(self.l6(h))\n",
        "        \n",
        "        out = F.tanh(self.l_out(h))\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.c0 = L.Convolution2D(3, 32, 5, 1, 2, initialW=w)\n",
        "            self.bn0 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c1 = L.Convolution2D(None, 32, 6, 2, 2, initialW=w)\n",
        "            self.bn1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c2 = L.Convolution2D(None, 64, 6, 2, 2, initialW=w)\n",
        "            self.bn2 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c3 = L.Convolution2D(None, 128, 6, 2, 2, initialW=w)\n",
        "            self.bn3 = L.BatchNormalization(128)            \n",
        "            \n",
        "            self.l3 = L.Linear(None, 128, initialW=w)     \n",
        "            self.t_32 = L.Linear(None, 32*4, initialW=None)\n",
        "            \n",
        "            self.c4 = L.Convolution2D(None, 256, 4, 2, 1, initialW=w)\n",
        "            self.bn4 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.c5 = L.Convolution2D(None, 512, 4, 2, 1, initialW=w)\n",
        "            self.bn5 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c6 = L.Convolution2D(None, 1024, 4, 2, 1, initialW=w)\n",
        "            self.bn6 = L.BatchNormalization(1024)\n",
        "            \n",
        "            self.l6 = L.Linear(None, 1024, initialW=w)     \n",
        "            self.t_4 = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def forward(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        h = F.leaky_relu(self.bn0(self.c0(F.gaussian(z, cupy.ones(z.shape, dtype=cupy.float32)*1e-1))))\n",
        "        h = F.leaky_relu(self.bn1(self.c1(F.gaussian(h, cupy.ones(h.shape, dtype=cupy.float32)*1e-1))))\n",
        "        h = F.leaky_relu(self.bn2(self.c2(F.gaussian(h, cupy.ones(h.shape, dtype=cupy.float32)*1e-1))))\n",
        "        h_32 = F.leaky_relu(self.bn3(self.c3(F.gaussian(h, cupy.ones(h.shape, dtype=cupy.float32)*1e-1))))\n",
        "        \n",
        "        h = self.l3(h_32)\n",
        "        m = F.reshape(self.t_32(h), (batchsize, 32, 4))\n",
        "        o_b_32 = self.minibatch_discrimination(m)\n",
        "        \n",
        "        h = F.leaky_relu(self.bn4(self.c4(F.gaussian(h_32, cupy.ones(h_32.shape, dtype=cupy.float32)*1e-1))))\n",
        "        h = F.leaky_relu(self.bn5(self.c5(F.gaussian(h, cupy.ones(h.shape, dtype=cupy.float32)*1e-1))))\n",
        "        h_4 = F.leaky_relu(self.bn6(self.c6(F.gaussian(h, cupy.ones(h.shape, dtype=cupy.float32)*1e-1))))\n",
        "        \n",
        "        h = self.l6(h_4)\n",
        "        m = F.reshape(self.t_4(h), (batchsize, 64, 16))\n",
        "        o_b_4 = self.minibatch_discrimination(m)\n",
        "        \n",
        "        h = F.concat((h, o_b_4, o_b_32), axis=1)\n",
        "        \n",
        "        return self.l_o(h), h_4, h_32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            w_out = chainer.initializers.HeNormal(wscale*1e-1)\n",
        "            \n",
        "            self.l_s32 = L.Linear(None, 128*128, initialW=w, nobias=True)\n",
        "            self.l_b32 = L.Linear(None, 32*32, initialW=w, nobias=True)\n",
        "            self.bn_32 = L.BatchNormalization(129)\n",
        "            \n",
        "            self.l_s128 = L.Linear(None, 64*32, initialW=w, nobias=True)\n",
        "            self.l_b128 = L.Linear(None, 128*128, initialW=w, nobias=True)\n",
        "            self.bn_128 = L.BatchNormalization(33)\n",
        "            \n",
        "            self.l_z4 = L.Linear(None, 2048*2*2, initialW=w, nobias=True)\n",
        "            self.dc1 = L.Deconvolution2D(None, 1024, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn1 = L.BatchNormalization(1024)\n",
        "            # 4\n",
        "            \n",
        "            self.dc2 = L.Deconvolution2D(None, 512, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn2 = L.BatchNormalization(512)\n",
        "            # 8\n",
        "            \n",
        "            self.dc3 = L.Deconvolution2D(None, 256, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn3 = L.BatchNormalization(256)\n",
        "            # 16\n",
        "            \n",
        "            self.dc4 = L.Deconvolution2D(None, 128, 4, 2, 1, initialW=w, nobias=True)\n",
        "            self.bn4 = L.BatchNormalization(128)\n",
        "            # 32\n",
        "            \n",
        "            self.dc5 = L.Deconvolution2D(None, 128, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn5 = L.BatchNormalization(128)\n",
        "            #64\n",
        "            \n",
        "            self.dc6 = L.Deconvolution2D(None, 64, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn6 = L.BatchNormalization(64)\n",
        "            # 128\n",
        "            \n",
        "            self.dc7 = L.Deconvolution2D(None, 32, 6, 2, 2, initialW=w, nobias=True)\n",
        "            self.bn7 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.c_out = L.Convolution2D(None, 3, 1, 1, 0, initialW=w)\n",
        "            # 256\n",
        "      \n",
        "    def mul(self, z, n):\n",
        "        batch, ch, w, h = z.shape\n",
        "        z = F.reshape(z, (batch, ch, -1))\n",
        "        _, _, ch = n.shape\n",
        "        z = F.batch_matmul(z, n, transa=True)\n",
        "        z = F.transpose(z, (0, 2, 1))\n",
        "        return F.reshape(z, (batch, ch, w, h))\n",
        "\n",
        "    def forward(self, z_2, z_32, z_128):\n",
        "        batch = z_2.shape[0]\n",
        "        h = F.reshape(F.relu(self.l_z4(z_2)) ,(batch, -1, 2, 2))\n",
        "        h = F.relu(self.bn1(self.dc1(h)))\n",
        "        # 4\n",
        "        \n",
        "        h = F.relu(self.bn2(self.dc2(h)))\n",
        "        # 8\n",
        "        \n",
        "        h = F.relu(self.bn3(self.dc3(h)))\n",
        "        # 16\n",
        "        \n",
        "        n = F.reshape(gaussian(batch*16*16), (batch, 1, 16, 16))\n",
        "        h = F.relu(self.bn4(self.dc4(F.concat((h, n)))))\n",
        "        \n",
        "        n = F.reshape(self.l_s32(z_32) ,(batch, 128, -1))\n",
        "        h = self.mul(h, n)\n",
        "        \n",
        "        n = F.reshape(self.l_b32(z_32) ,(-1, 1, 32, 32))\n",
        "        h = self.bn_32(F.concat((h, n)))\n",
        "        # 32\n",
        "        \n",
        "        h = F.relu(self.bn5(self.dc5(h)))\n",
        "        # 64\n",
        "        \n",
        "        n = F.reshape(gaussian(batch*64*64), (batch, 1, 64, 64))\n",
        "        h = F.relu(self.bn6(self.dc6(F.concat((h, n)))))\n",
        "        \n",
        "        n = F.reshape(self.l_s128(z_128) ,(batch, 64, -1))\n",
        "        h = self.mul(h, n)\n",
        "        \n",
        "        n = F.reshape(self.l_b128(z_128) ,(-1, 1, 128, 128))\n",
        "        h = self.bn_128(F.concat((h, n)))\n",
        "        # 128\n",
        "        \n",
        "        h = F.relu(self.bn7(self.dc7(h)))\n",
        "        # 256\n",
        "        \n",
        "        h = F.tanh(self.c_out(h))\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTYXPf-Gcyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, encoder_deep, encoder_middle, affine_transfer, decoder):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder_deep = encoder_deep\n",
        "        self.encoder_middle = encoder_middle\n",
        "        self.affine_transfer = affine_transfer\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x_c, x_v):\n",
        "        batch = x_c.shape[0]\n",
        "        mu_c, var_c = self.encoder_deep(x_c)\n",
        "        h_c = F.gaussian(mu_c, var_c)\n",
        "        h_v = self.encoder_middle(x_v)\n",
        "        h_v = self.affine_transfer(h_v)\n",
        "        s_128 = F.reshape(gaussian(batch*16), (-1, 16))\n",
        "        return self.decoder(h_c, h_v, s_128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkPjj5gHE76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis= kwargs.pop('models')\n",
        "        self.cache_fake = None\n",
        "        self.cache_dis = None\n",
        "        self.k = 5\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "        \n",
        "    def gram(self, x):\n",
        "        batch, ch, height, width = x.shape\n",
        "        feature = F.reshape(x, (batch, ch, -1))\n",
        "        return F.batch_matmul(feature, feature, transb=True) / (ch*height*width)\n",
        "\n",
        "    def loss_dis(self, dis, ll, opt):\n",
        "        loss = sum(ll)\n",
        "        chainer.report({opt: loss}, dis)\n",
        "        return loss\n",
        "    \n",
        "    def loss_en(self, en, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'lat': ll[1]}, en)\n",
        "        return loss\n",
        "    \n",
        "    def loss_de(self, de, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'fake': ll[0]}, de)\n",
        "        return loss\n",
        "\n",
        "    def update_core(self):\n",
        "        en_d_optimizer = self.get_optimizer('en_d')\n",
        "        en_m_optimizer = self.get_optimizer('en_m')\n",
        "        affine_t_optimizer = self.get_optimizer('affine_t')\n",
        "        dis_optimizer = self.get_optimizer('dis')\n",
        "        de_optimizer = self.get_optimizer('de')\n",
        "        \n",
        "        gen, dis = self.gen, self.dis\n",
        "        \n",
        "        for _ in range(self.k):\n",
        "            batch_c = self.get_iterator('main').next()\n",
        "            batchsize = len(batch_c)\n",
        "            x_c_real = Variable(self.converter(batch_c, device=self.device)) /255. *2. -1.\n",
        "\n",
        "            batch_v = self.get_iterator('middle').next()\n",
        "            x_v_real = Variable(self.converter(batch_v, device=self.device)) /255. *2. -1.\n",
        "            \n",
        "            mu_c_real, var_c_real = gen.encoder_deep(x_c_real)\n",
        "            h_c_real = F.gaussian(mu_c_real, var_c_real)\n",
        "\n",
        "            h_v_real = gen.encoder_middle(x_v_real)\n",
        "            h_v_at = gen.affine_transfer(h_v_real)\n",
        "\n",
        "            s_128_real = F.reshape(gaussian(batchsize*16), (-1, 16))\n",
        "\n",
        "            x_fake = gen.decoder(h_c_real, h_v_at, s_128_real)\n",
        "            \n",
        "            y_c_real, fm4_c_real, _ = dis(x_c_real)\n",
        "            dis_loss_real = F.sum((y_c_real-1)**2) / batchsize\n",
        "            dis_optimizer.update(self.loss_dis, dis, [dis_loss_real], 'c_real')\n",
        "\n",
        "            y_v_real, _, fm32_v_real = dis(x_v_real)\n",
        "            dis_loss_real = F.sum((y_v_real-1)**2) / batchsize\n",
        "            dis_optimizer.update(self.loss_dis, dis, [dis_loss_real], 'v_real')\n",
        "            \n",
        "            y_fake, fm4_fake, fm32_fake = dis(x_fake)\n",
        "            dis_loss_fake = F.sum(y_fake**2) / batchsize\n",
        "            dis_optimizer.update(self.loss_dis, dis, [dis_loss_fake], 'fake')\n",
        "        \n",
        "            if self.cache_fake is not None:\n",
        "                remind, _, _ = dis(self.cache_fake)\n",
        "                dis_loss_remind = F.sum(remind**2) / batchsize\n",
        "                dis_optimizer.update(self.loss_dis, dis, [dis_loss_remind], 'remind')\n",
        "                \n",
        "            if self.cache_dis is None:\n",
        "                self.cache_dis = dis.copy('copy')\n",
        "                \n",
        "        dis.copyparams(self.cache_dis)\n",
        "        self.cache_dis = None\n",
        "        \n",
        "        latent_loss = F.mean(F.gaussian_kl_divergence(mu_c_real, var_c_real)) / batchsize *1e-3\n",
        "        fake_loss = F.sum((y_fake-1)**2) / batchsize\n",
        "        fm4_loss = F.mean_squared_error(fm4_c_real, fm4_fake)\n",
        "        fm32_loss = F.mean_squared_error(fm32_v_real, fm32_fake)\n",
        "        \n",
        "        en_d_optimizer.update(self.loss_en, gen.encoder_deep, [fm4_loss, latent_loss])\n",
        "        en_m_optimizer.update(self.loss_de, gen.encoder_middle, [fm32_loss])\n",
        "        affine_t_optimizer.update(self.loss_de, gen.affine_transfer, [fm32_loss])\n",
        "        de_optimizer.update(self.loss_de, gen.decoder, [fake_loss, fm4_loss, fm32_loss])\n",
        "        \n",
        "        self.cache_fake = x_fake\n",
        "        \n",
        "        # style_loss = F.mean_squared_error(self.gram(x_real), self.gram(x_fake))\n",
        "        # reconst_loss = F.mean(F.bernoulli_nll(x_c_real, x_fake)) / batchsize *1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifY8to6Hhsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = 0\n",
        "OUT = mount+'/My Drive/result/'\n",
        "IMG_SHAPE = (256, 256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iteration = 500\n",
        "batchsize = 16\n",
        "snapshot_interval = 20\n",
        "display_interval = 20\n",
        "update_interval = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6bmkEnHoQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = mount+'/My Drive/picture/train_pic/**/*.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49zXHF0QHq6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(encoder_deep=EncoderDeep(), encoder_middle=EncoderMiddle(), affine_transfer=AffineTransfer(), decoder=Decoder())\n",
        "dis = Discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6m896tHwsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chainer.backends.cuda.get_device_from_id(GPU).use()\n",
        "gen.to_gpu()\n",
        "gen.encoder_deep.to_gpu()\n",
        "gen.encoder_middle.to_gpu()\n",
        "gen.affine_transfer.to_gpu()\n",
        "gen.decoder.to_gpu()\n",
        "dis.to_gpu()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBvJVx3H8xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_en_d = make_optimizer_Adam(gen.encoder_deep, clip=False)\n",
        "opt_en_m = make_optimizer_Adam(gen.encoder_middle, clip=False)\n",
        "opt_affine_t = make_optimizer_Adam(gen.affine_transfer, clip=False)\n",
        "opt_de = make_optimizer_Adam(gen.decoder, clip=False)\n",
        "opt_dis = make_optimizer_Adam(dis, clip=False)\n",
        "dis.t_4.disable_update()\n",
        "dis.t_32.disable_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3yTVR2tGV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sq(img):\n",
        "    img_width, img_height = img.size\n",
        "    sub = img_width - img_height\n",
        "    if sub == 0:\n",
        "        return img\n",
        "    crop_size = min(img.size)\n",
        "    crop_position = numpy.random.randint(0, abs(sub))\n",
        "    if sub > 0:\n",
        "        return img.crop((crop_position, 0, crop_size+crop_position, crop_size))\n",
        "    else:\n",
        "        return img.crop((0, crop_position, crop_size, crop_size+crop_position))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr27raJaIDGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_convert(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    width, height = img.size\n",
        "    image_trans_trigger = numpy.random.randn(2)\n",
        "    \n",
        "    img = crop_sq(img)\n",
        "        \n",
        "    if image_trans_trigger[0] > 0:\n",
        "        img = ImageOps.mirror(img)\n",
        "        \n",
        "    if image_trans_trigger[1] == 0.:\n",
        "        img = img.resize(IMG_SHAPE)\n",
        "    else:\n",
        "        img = img.resize((IMG_SHAPE[0]+64, IMG_SHAPE[1]+64))\n",
        "        crop_position = numpy.random.randint(0, 64, 2)\n",
        "        img = img.crop((crop_position[0],\n",
        "                       crop_position[1],\n",
        "                       crop_position[0]+IMG_SHAPE[0],\n",
        "                       crop_position[1]+IMG_SHAPE[1]))\n",
        "\n",
        "    out_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1l6Zuw9IHKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_resize(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    img = crop_sq(img)\n",
        "      \n",
        "    img = img.resize(IMG_SHAPE)\n",
        "    out_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xynzSKG4IKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_files = glob.glob(dataset, recursive=True)\n",
        "print('{} contains {} image files'\n",
        "      .format(dataset, len(image_files)))\n",
        "img_dataset = chainer.datasets.ImageDataset(paths=image_files)\n",
        "\n",
        "train = chainer.datasets.TransformDataset(img_dataset, img_convert)\n",
        "sample = chainer.datasets.TransformDataset(img_dataset, img_resize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsS_Vf0KIOQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_c_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)\n",
        "train_v_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToPAI31hIQoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "    models=(gen, dis),\n",
        "    iterator={'main': train_v_iter, 'middle': train_c_iter},\n",
        "    optimizer={'de': opt_de, 'en_d': opt_en_d, 'en_m': opt_en_m, 'affine_t': opt_affine_t, 'dis': opt_dis},\n",
        "    device=GPU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5phMAnIWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = training.Trainer(updater, (iteration, 'iteration'), out=OUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqcOBLPUIaSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o2TkGb1Icnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(out_generated_image(gen,\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True),\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True)),\n",
        "               trigger=snapshot_interval)\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'en_d/loss', 'en_d/lat', 'en_m/loss', 'dis/c_real', 'dis/v_real', 'dis/fake', 'de/loss', 'de/fake'\n",
        "    ]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=update_interval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNdaVJ4gy9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if True:\n",
        "    chainer.serializers.load_npz(OUT+'hobo_en_d.npz', gen.encoder_deep, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_en_m.npz', gen.encoder_middle, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_affine_t.npz', gen.affine_transfer, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_de.npz', gen.decoder, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_dis.npz', dis, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_opt_en_d.npz', opt_en_d, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_opt_en_m.npz', opt_en_m, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_opt_affine_t.npz', opt_affine_t, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_opt_de.npz', opt_de, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'hobo_opt_dis.npz', opt_dis, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epn0ZUHekKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if True:\n",
        "    chainer.serializers.save_npz(OUT+'hobo_en_d.npz', gen.encoder_deep)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_en_m.npz', gen.encoder_middle)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_affine_t.npz', gen.affine_transfer)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_de.npz', gen.decoder)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_dis.npz', dis)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_opt_en_d.npz', opt_en_d)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_opt_en_m.npz', opt_en_m)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_opt_affine_t.npz', opt_affine_t)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_opt_de.npz', opt_de)\n",
        "    chainer.serializers.save_npz(OUT+'hobo_opt_dis.npz', opt_dis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INO_uTsqkfIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}