{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GauGANbukkorosuGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "outputId": "3f2e9cd8-9b7a-4510-9012-c47293e2b503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "iteration = 100\n",
        "batchsize = 16\n",
        "snapshot_interval = 20\n",
        "display_interval = 20\n",
        "update_interval = 20\n",
        "phase = 0\n",
        "'''\n",
        "0- : all decoder training.\n",
        "1- : trans encoding.\n",
        "2- : adversarial encording.\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n0- : all decoder training.\\n1- : trans encoding.\\n2- : adversarial encording.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnDuRC0v0Ru",
        "colab_type": "code",
        "outputId": "1d9a8670-45a7-42e3-8da4-a2a48aaac249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mount = '/content/gdrive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(mount)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGFtpuE3Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import math\n",
        "import glob\n",
        "from PIL import Image, ImageOps, ImageChops, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy\n",
        "\n",
        "import chainer\n",
        "from chainer import training, backend, Variable\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import chainer.backends.cuda\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGloZh9xEu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(size):\n",
        "    return F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                     cupy.ones(size, dtype=cupy.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9P2_zkbFjeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer_Adam(model, alpha=1e-4, beta1=0.9, clip=True):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n",
        "    optimizer.setup(model)\n",
        "    if clip:\n",
        "        optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(1.))\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def out_generated_image(gen, c_iter, v_iter):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        clear_output()\n",
        "        xp = gen.xp\n",
        "        c_base = xp.asarray(c_iter.next())\n",
        "        z_c = Variable(c_base[:,1])/255. *2. -1.\n",
        "        if phase < 7:\n",
        "            v_base = c_base[:,0]\n",
        "        else:\n",
        "            v_base = xp.asarray(v_iter.next())[:,0]\n",
        "        z_v = Variable(v_base)/255. *2. -1.\n",
        "        \n",
        "        with chainer.using_config('train', False):\n",
        "            x = gen(z_c, z_v)\n",
        "        x = F.transpose(F.reshape(x, (-1, 3, 128, 128)), (0, 2, 3, 1))\n",
        "        x = chainer.backends.cuda.to_cpu(x.array)\n",
        "        \n",
        "        plt.figure(figsize=(16, 12))\n",
        "        \n",
        "        \n",
        "        for i, img in enumerate(xp.asnumpy(c_base[:,1])):\n",
        "            plt.subplot(3,4,i+1)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "            \n",
        "        for i, img in enumerate(x):\n",
        "            plt.subplot(3,4,i+5)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "            \n",
        "        for i, img in enumerate(xp.asnumpy(v_base)):\n",
        "            plt.subplot(3,4,i+9)\n",
        "            plt.axis('off')\n",
        "            plt.imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "        \n",
        "        plt.show()\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYAW64atFmub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.c0_0 = L.Convolution2D(3, 64, 1, 1, 0, initialW=w)\n",
        "            self.bn0_0 = L.BatchNormalization(64)\n",
        "            self.c0_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn0_1 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c1_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn1_0 = L.BatchNormalization(128)\n",
        "            self.c1_1 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_1 = L.BatchNormalization(128)\n",
        "            self.c1_2 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_2 = L.BatchNormalization(128)\n",
        "            self.c1_3 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_3 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c2_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn2_0 = L.BatchNormalization(256)\n",
        "            self.c2_1 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.bn2_1 = L.BatchNormalization(256)\n",
        "            self.c2_2 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.bn2_2 = L.BatchNormalization(256)\n",
        "            self.c2_3 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.bn2_3 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.c3_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn3_0 = L.BatchNormalization(512)\n",
        "            self.c3_1 = L.Convolution2D(None, 512, 1, 1, 0, initialW=w)\n",
        "            self.bn3_1 = L.BatchNormalization(512)\n",
        "            self.c3_2 = L.Convolution2D(None, 512, 1, 1, 0, initialW=w)\n",
        "            self.bn3_2 = L.BatchNormalization(512)\n",
        "            self.c3_3 = L.Convolution2D(None, 512, 1, 1, 0, initialW=w)\n",
        "            self.bn3_3 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c4_0 = L.Convolution2D(None, 1024, 3, 1, 1, initialW=w)\n",
        "            self.bn4_0 = L.BatchNormalization(1024)\n",
        "            self.c4_1 = L.Convolution2D(None, 1024, 1, 1, 0, initialW=w)\n",
        "            self.bn4_1 = L.BatchNormalization(1024)\n",
        "            self.c4_2 = L.Convolution2D(None, 1024, 1, 1, 0, initialW=w)\n",
        "            self.bn4_2 = L.BatchNormalization(1024)\n",
        "            self.c4_3 = L.Convolution2D(None, 1024, 1, 1, 0, initialW=w)\n",
        "            self.bn4_3 = L.BatchNormalization(1024)\n",
        "            \n",
        "            self.l1 = L.Linear(None, 1024, initialW=w)\n",
        "            self.l2 = L.Linear(None, 1024, initialW=w)\n",
        "            self.l_out = L.Linear(None, 1024, initialW=w)\n",
        "    \n",
        "    def self_attention(self, x, f, g, h, gamma=0.01):\n",
        "        f_shape = f.shape\n",
        "        g_shape = g.shape\n",
        "        h_shape = h.shape\n",
        "        \n",
        "        attention_map = F.batch_matmul(F.reshape(f, (-1, f_shape[2], f_shape[3])),\n",
        "                                       F.reshape(g, (-1, g_shape[2], g_shape[3])),\n",
        "                                       transb=True)\n",
        "        feature_map = F.batch_matmul(F.reshape(h, (-1, h_shape[2], h_shape[3])),\n",
        "                                     attention_map, transb=True)\n",
        "        feature_map = F.reshape(feature_map, h_shape)\n",
        "        return F.add(x, feature_map*gamma)\n",
        "    \n",
        "    def gap(self, x):\n",
        "        return F.average_pooling_2d(x, x.shape[3])\n",
        "    \n",
        "    def forward(self, z):\n",
        "        batch = z.shape[0]\n",
        "        x = F.relu(self.bn0_0(self.c0_0(z)))\n",
        "        x = F.relu(self.bn0_1(self.c0_1(x)))\n",
        "        # 128\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.relu(self.bn1_0(self.c1_0(x)))\n",
        "        f = F.tanh(self.bn1_1(self.c1_1(x)))\n",
        "        g = F.tanh(self.bn1_2(self.c1_2(x)))\n",
        "        h = F.tanh(self.bn1_3(self.c1_3(x)))\n",
        "        x = F.relu(self.self_attention(x, f, g, h))\n",
        "        # 64\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.relu(self.bn2_0(self.c2_0(x)))\n",
        "        f = F.tanh(self.bn2_1(self.c2_1(x)))\n",
        "        g = F.tanh(self.bn2_2(self.c2_2(x)))\n",
        "        h = F.tanh(self.bn2_3(self.c2_3(x)))\n",
        "        x = F.relu(self.self_attention(x, f, g, h))\n",
        "        # 32\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.relu(self.bn3_0(self.c3_0(x)))\n",
        "        f = F.tanh(self.bn3_1(self.c3_1(x)))\n",
        "        g = F.tanh(self.bn3_2(self.c3_2(x)))\n",
        "        h = F.tanh(self.bn3_3(self.c3_3(x)))\n",
        "        x = F.relu(self.self_attention(x, f, g, h))\n",
        "        # 16\n",
        "        x = F.max_pooling_2d(x, 2)\n",
        "        x = F.relu(self.bn4_0(self.c4_0(x)))\n",
        "        f = F.tanh(self.bn4_1(self.c4_1(x)))\n",
        "        g = F.tanh(self.bn4_2(self.c4_2(x)))\n",
        "        h = F.tanh(self.bn4_3(self.c4_3(x)))\n",
        "        x = F.relu(self.self_attention(x, f, g, h))\n",
        "        # 8\n",
        "        x = self.gap(x)\n",
        "        \n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l_out(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorGen(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(DiscriminatorGen, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.c0 = L.Convolution2D(3, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn0 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn1 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c2_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn2_0 = L.BatchNormalization(128)\n",
        "            self.c2_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn2_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c3_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn3_0 = L.BatchNormalization(256)\n",
        "            self.c3_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn3_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn4_0 = L.BatchNormalization(512)\n",
        "            self.c4_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn4_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn5_0 = L.BatchNormalization(512)\n",
        "            self.c5_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn5_1= L.BatchNormalization(256)\n",
        "            \n",
        "            self.c6_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn6_0 = L.BatchNormalization(512)\n",
        "            self.c6_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn6_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.cb = L.Convolution2D(None, 512, 1, 1, 0, initialW=w)\n",
        "            self.bnb = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c7 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn7 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c8 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn8 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c9 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.bn9 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.l_m = L.Linear(None, 1024, initialW=w)\n",
        "            self.t = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        batchsize = m.shape[0]\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def gaussian(self, x):\n",
        "        return F.gaussian(x, cupy.ones(x.shape, dtype=cupy.float32)*1e-1)\n",
        "    \n",
        "    def forward(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        h = F.leaky_relu(self.bn0(self.c0(self.gaussian(z))))\n",
        "        h = F.leaky_relu(self.bn1(self.c1(self.gaussian(h))))\n",
        "        # 128\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bn2_0(self.c2_0(self.gaussian(h))))\n",
        "        h = F.leaky_relu(self.bn2_1(self.c2_1(self.gaussian(h))))\n",
        "        # 64\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bn3_0(self.c3_0(self.gaussian(h))))\n",
        "        h = F.leaky_relu(self.bn3_1(self.c3_1(self.gaussian(h))))\n",
        "        # 32\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bn4_0(self.c4_0(self.gaussian(h))))\n",
        "        h = F.leaky_relu(self.bn4_1(self.c4_1(self.gaussian(h))))\n",
        "        h_16 = h\n",
        "        # 16\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bn5_0(self.c5_0(self.gaussian(h))))\n",
        "        h = F.leaky_relu(self.bn5_1(self.c5_1(self.gaussian(h))))\n",
        "        h_8 = h\n",
        "        # 8\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bn6_0(self.c6_0(self.gaussian(h))))\n",
        "        h = F.leaky_relu(self.bn6_1(self.c6_1(self.gaussian(h))))\n",
        "        h_4 = h\n",
        "        # 4\n",
        "        h = F.space2depth(h, 2)\n",
        "        h = F.leaky_relu(self.bnb(self.cb(self.gaussian(h))))\n",
        "        # 2\n",
        "        \n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, h_4))\n",
        "        h = F.leaky_relu(self.bn7(self.c7(h)))\n",
        "        # 4\n",
        "        \n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, h_8))\n",
        "        h = F.leaky_relu(self.bn8(self.c8(h)))\n",
        "        # 8\n",
        "        \n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, h_16))\n",
        "        h = F.leaky_relu(self.bn9(self.c9(h)))\n",
        "        # 16\n",
        "        \n",
        "        h_m = F.leaky_relu(self.l_m(h))\n",
        "        m = F.reshape(self.t(h_m), (batchsize, 64, 16))\n",
        "        o_b = self.minibatch_discrimination(m)\n",
        "        \n",
        "        h = F.concat((h_m, o_b), axis=1)\n",
        "        \n",
        "        return self.l_o(h), h_m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXTfbl5A3P7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorEn(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(DiscriminatorEn, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.l0 = L.Linear(None, 1024, initialW=w)\n",
        "            self.l1 = L.Linear(None, 1024, initialW=w)\n",
        "            self.t = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        batchsize = m.shape[0]\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def forward(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        h = F.relu(self.l0(z))\n",
        "        h = F.relu(self.l1(h))\n",
        "        m = F.reshape(self.t(h), (batchsize, 64, 16))\n",
        "        o_b = self.minibatch_discrimination(m)\n",
        "        \n",
        "        out = F.concat((h, o_b), axis=1)\n",
        "        \n",
        "        return self.l_o(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=1.):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.lsg = L.Linear(None, 128*128, initialW=w)\n",
        "            self.lsb = L.Linear(None, 128, initialW=w)\n",
        "            \n",
        "            self.enc0_0 = L.Convolution2D(3, 32, 3, 1, 1, initialW=w)\n",
        "            self.enbnc0_0 = L.BatchNormalization(32)\n",
        "            self.enc0_1 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc0_1 = L.BatchNormalization(32)\n",
        "            self.enc0_2 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc0_2 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.enc1_0 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.enbnc1_0 = L.BatchNormalization(32)\n",
        "            self.enc1_1 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc1_1 = L.BatchNormalization(32)\n",
        "            self.enc1_2 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc1_2 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.enc2_0 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.enbnc2_0 = L.BatchNormalization(32)\n",
        "            self.enc2_1 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc2_1 = L.BatchNormalization(32)\n",
        "            self.enc2_2 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc2_2 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.enc3_0 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.enbnc3_0 = L.BatchNormalization(32)\n",
        "            self.enc3_1 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc3_1 = L.BatchNormalization(32)\n",
        "            self.enc3_2 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.enbnc3_2 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.enc4 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.enbnc4 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.de4_dc5 = L.Deconvolution2D(None, 256, 5, 1, 2, initialW=w)\n",
        "            self.debn4_dc5 = L.BatchNormalization(256)\n",
        "            self.de4_c3 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.debn4_c3 = L.BatchNormalization(128)\n",
        "            self.de4_c1 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.debn4_c1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.de3_dc5 = L.Deconvolution2D(None, 128, 5, 1, 2, initialW=w)\n",
        "            self.debn3_dc5 = L.BatchNormalization(128)\n",
        "            self.de3_c3 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.debn3_c3 = L.BatchNormalization(64)\n",
        "            self.de3_c1 = L.Convolution2D(None, 64, 1, 1, 0, initialW=w)\n",
        "            self.debn3_c1 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.de2_dc5 = L.Deconvolution2D(None, 64, 5, 1, 2, initialW=w)\n",
        "            self.debn2_dc5 = L.BatchNormalization(64)\n",
        "            self.de2_c3 = L.Convolution2D(None, 32, 3, 1, 1, initialW=w)\n",
        "            self.debn2_c3 = L.BatchNormalization(32)\n",
        "            self.de2_c1 = L.Convolution2D(None, 32, 1, 1, 0, initialW=w)\n",
        "            self.debn2_c1 = L.BatchNormalization(32)\n",
        "            \n",
        "            self.de1_dc5 = L.Deconvolution2D(None, 32, 5, 1, 2, initialW=w)\n",
        "            self.debn1_dc5 = L.BatchNormalization(32)\n",
        "            self.de1_c3 = L.Convolution2D(None, 16, 3, 1, 1, initialW=w)\n",
        "            self.debn1_c3 = L.BatchNormalization(16)\n",
        "            self.de1_c1 = L.Convolution2D(None, 16, 1, 1, 0, initialW=w)\n",
        "            self.debn1_c1 = L.BatchNormalization(16)\n",
        "            \n",
        "            self.c_out = L.Convolution2D(None, 3, 1, 1, 0, initialW=w)\n",
        "    \n",
        "    def mul(self, a, b):\n",
        "        a_shape = a.shape\n",
        "        b_shape = b.shape\n",
        "        \n",
        "        x = F.batch_matmul(F.reshape(a, (-1, a_shape[2], a_shape[3])),\n",
        "                                       F.reshape(b, (-1, b_shape[2], b_shape[3])),\n",
        "                                       transb=True)\n",
        "        return F.reshape(x, a_shape)\n",
        "    \n",
        "    def style_gamma(self, x, gamma):\n",
        "        batch, ch, w, h = x.shape\n",
        "        z = F.reshape(x, (batch, ch, -1))\n",
        "        _, _, ch = gamma.shape\n",
        "        z = F.batch_matmul(z, gamma, transa=True)\n",
        "        z = F.transpose(z, (0, 2, 1))\n",
        "        return F.reshape(z, (batch, ch, w, h))\n",
        "    \n",
        "    def gaussian(self, batch, ch, w, h):\n",
        "        size = batch*ch*w*h\n",
        "        noise = F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                           cupy.ones(size, dtype=cupy.float32))\n",
        "        return F.reshape(noise, (batch, ch, w, h))\n",
        "    \n",
        "    def forward(self, x, z):        \n",
        "        batch = x.shape[0]\n",
        "        \n",
        "        # Encode\n",
        "        f = F.tanh(self.enbnc0_0(self.enc0_0(x)))\n",
        "        g = F.tanh(self.enbnc0_1(self.enc0_1(x)))\n",
        "        b = F.tanh(self.enbnc0_2(self.enc0_2(x)))\n",
        "        h = self.mul(f, g)\n",
        "        h = F.add(h, b)\n",
        "        h = F.relu(h)\n",
        "        # 128\n",
        "        h = F.space2depth(h, 2)\n",
        "        f = F.tanh(self.enbnc1_0(self.enc1_0(h)))\n",
        "        g = F.tanh(self.enbnc1_1(self.enc1_1(h)))\n",
        "        b = F.tanh(self.enbnc1_2(self.enc1_2(h)))\n",
        "        h = self.mul(f, g)\n",
        "        h = F.add(h, b)\n",
        "        h = F.relu(h)\n",
        "        # 64\n",
        "        h = F.space2depth(h, 2)\n",
        "        f = F.tanh(self.enbnc2_0(self.enc2_0(h)))\n",
        "        g = F.tanh(self.enbnc2_1(self.enc2_1(h)))\n",
        "        b = F.tanh(self.enbnc2_2(self.enc2_2(h)))\n",
        "        h = self.mul(f, g)\n",
        "        h = F.add(h, b)\n",
        "        h = F.relu(h)\n",
        "        # 32\n",
        "        h = F.space2depth(h, 2)\n",
        "        f = F.tanh(self.enbnc3_0(self.enc3_0(h)))\n",
        "        g = F.tanh(self.enbnc3_1(self.enc3_1(h)))\n",
        "        b = F.tanh(self.enbnc3_2(self.enc3_2(h)))\n",
        "        h = self.mul(f, g)\n",
        "        h = F.add(h, b)\n",
        "        h = F.relu(h)\n",
        "        h = F.tanh(self.enbnc4(self.enc4(h)))\n",
        "        \n",
        "        sg = F.tanh(self.lsg(z))\n",
        "        sg = F.reshape(sg, (batch, 128, 128))\n",
        "        h = self.style_gamma(h, sg)\n",
        "        \n",
        "        sb = F.tanh(self.lsb(z))\n",
        "        sb = F.broadcast_to(F.reshape(sb, (batch, 128, 1, 1)),\n",
        "                            (batch, 128, 16, 16))\n",
        "        h = F.add(h, sb)\n",
        "        h = F.relu(h)\n",
        "        # 16\n",
        "        \n",
        "        # Decode\n",
        "        h = F.concat((h, self.gaussian(batch, 1, 16, 16)))\n",
        "        h_dc5 = F.relu(self.debn4_dc5(self.de4_dc5(h)))\n",
        "        h_c3 = F.relu(self.debn4_c3(self.de4_c3(h_dc5)))\n",
        "        h_c1 = F.relu(self.debn4_c1(self.de4_c1(h_c3)))\n",
        "        h = F.concat((h_dc5, h_c3, h_c1))\n",
        "        # 16\n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, self.gaussian(batch, 1, 32, 32)))\n",
        "        h_dc5 = F.relu(self.debn3_dc5(self.de3_dc5(h)))\n",
        "        h_c3 = F.relu(self.debn3_c3(self.de3_c3(h_dc5)))\n",
        "        h_c1 = F.relu(self.debn3_c1(self.de3_c1(h_c3)))\n",
        "        h = F.concat((h_dc5, h_c3, h_c1))\n",
        "        # 32\n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, self.gaussian(batch, 1, 64, 64)))\n",
        "        h_dc5 = F.relu(self.debn2_dc5(self.de2_dc5(h)))\n",
        "        h_c3 = F.relu(self.debn2_c3(self.de2_c3(h_dc5)))\n",
        "        h_c1 = F.relu(self.debn2_c1(self.de2_c1(h_c3)))\n",
        "        h = F.concat((h_dc5, h_c3, h_c1))\n",
        "        # 64\n",
        "        h = F.depth2space(h, 2)\n",
        "        h = F.concat((h, self.gaussian(batch, 1, 128, 128)))\n",
        "        h_dc5 = F.relu(self.debn1_dc5(self.de1_dc5(h)))\n",
        "        h_c3 = F.relu(self.debn1_c3(self.de1_c3(h_dc5)))\n",
        "        h_c1 = F.relu(self.debn1_c1(self.de1_c1(h_c3)))\n",
        "        h = F.concat((h_dc5, h_c3, h_c1))\n",
        "        \n",
        "        h = F.tanh(self.c_out(h))\n",
        "        # 128\n",
        "        \n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xTYXPf-Gcyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Generator, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x_c, x_v):\n",
        "        batch = x_c.shape[0]\n",
        "        h_v = self.encoder(x_v)\n",
        "        return self.decoder(x_c, h_v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkPjj5gHE76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis_gen, self.dis_en = kwargs.pop('models')\n",
        "        self.cache_fake = None\n",
        "        self.cache_dis_gen = None\n",
        "        self.cache_dis_en = None\n",
        "        self.k = 5\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "        \n",
        "    def style_reconst(self, x, img_shape=128):\n",
        "        x_128 = F.reshape(x, (-1, img_shape, img_shape))\n",
        "        x_gram = F.flatten(F.batch_matmul(x_128, x_128, transb=True))/img_shape\n",
        "        \n",
        "        x_64 = F.average_pooling_2d(x, 2)\n",
        "        x_64 = F.reshape(x_64, (-1, x_64.shape[2], x_64.shape[3]))\n",
        "        x_64_gram = F.flatten(F.batch_matmul(x_64, x_64, transb=True))/x_64.shape[2]\n",
        "        x_gram = F.hstack((x_gram, x_64_gram))\n",
        "        \n",
        "        x_32 = F.average_pooling_2d(x, 2)\n",
        "        x_32 = F.reshape(x_32, (-1, x_32.shape[2], x_32.shape[3]))\n",
        "        x_32_gram = F.flatten(F.batch_matmul(x_32, x_32, transb=True))/x_32.shape[2]\n",
        "        x_gram = F.hstack((x_gram, x_32_gram))\n",
        "        \n",
        "        x_16 = F.average_pooling_2d(x, 2)\n",
        "        x_16 = F.reshape(x_16, (-1, x_16.shape[2], x_16.shape[3]))\n",
        "        x_16_gram = F.flatten(F.batch_matmul(x_16, x_16, transb=True))/x_16.shape[2]\n",
        "        x_gram = F.hstack((x_gram, x_16_gram))\n",
        "        \n",
        "        x_8 = F.average_pooling_2d(x, 2)\n",
        "        x_8 = F.reshape(x_8, (-1, x_8.shape[2], x_8.shape[3]))\n",
        "        x_8_gram = F.flatten(F.batch_matmul(x_8, x_8, transb=True))/x_8.shape[2]\n",
        "        x_gram = F.hstack((x_gram, x_8_gram))\n",
        "        \n",
        "        x_4 = F.average_pooling_2d(x, 2)\n",
        "        x_4 = F.reshape(x_4, (-1, x_4.shape[2], x_4.shape[3]))\n",
        "        x_4_gram = F.flatten(F.batch_matmul(x_4, x_4, transb=True))/x_4.shape[2]\n",
        "        x_gram = F.hstack((x_gram, x_4_gram))\n",
        "        \n",
        "        return x_gram\n",
        "\n",
        "    def loss_dis(self, dis, ll, opt):\n",
        "        loss = sum(ll)\n",
        "        chainer.report({opt: loss}, dis)\n",
        "        return loss\n",
        "    \n",
        "    def loss_en(self, en, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'style_rec': ll[-1]}, en)\n",
        "        return loss\n",
        "    \n",
        "    def loss_de(self, de, ll):\n",
        "        loss = sum(ll) \n",
        "        chainer.report({'loss': loss, 'fake': ll[0], 'rec': ll[-1]}, de)\n",
        "        return loss\n",
        "\n",
        "    def update_core(self):\n",
        "        en_optimizer = self.get_optimizer('en')\n",
        "        dis_gen_optimizer = self.get_optimizer('dis_gen')\n",
        "        dis_en_optimizer = self.get_optimizer('dis_en')\n",
        "        de_optimizer = self.get_optimizer('de')\n",
        "        \n",
        "        gen, dis_gen, dis_en = self.gen, self.dis_gen, self.dis_en\n",
        "        main_iter = self.get_iterator('main')\n",
        "        style_iter = self.get_iterator('style')\n",
        "        \n",
        "        for _ in range(self.k):\n",
        "            batch_c = main_iter.next()\n",
        "            batchsize = len(batch_c)\n",
        "            x_c = Variable(self.converter(batch_c, device=self.device)) /255. *2. -1.\n",
        "            x_c_real = x_c[:,0]\n",
        "            x_c_base = x_c[:,1]\n",
        "\n",
        "            batch_v = style_iter.next()\n",
        "            x_v_real = Variable(self.converter(batch_v, device=self.device))[:,0] /255. *2. -1.\n",
        "            \n",
        "            if phase >= 1:\n",
        "                h_v_real = gen.encoder(x_v_real)\n",
        "            else:\n",
        "                h_v_real = gen.encoder(x_c_real)\n",
        "            \n",
        "            if phase >= 2:\n",
        "                hy_real = dis_en(F.reshape(gaussian(batchsize*1024), (-1, 1024)))\n",
        "                dis_en_loss_real = F.sum((hy_real-1)**2) / batchsize\n",
        "                \n",
        "                hy_v_fake = dis_en(h_v_real)\n",
        "                dis_en_loss_v_fake = F.sum(hy_v_fake**2) / batchsize\n",
        "                dis_en_optimizer.update(self.loss_dis, dis_en,\n",
        "                                        [dis_en_loss_v_fake, dis_en_loss_real],\n",
        "                                        'v_loss')\n",
        "\n",
        "            x_fake = gen.decoder(x_c_base, h_v_real)\n",
        "            \n",
        "            y_c_real, fm_c_real = dis_gen(x_c_real)\n",
        "            dis_loss_real = F.sum((y_c_real-1)**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_real], 'c_real')\n",
        "\n",
        "            y_v_real, fm_v_real = dis_gen(x_v_real)\n",
        "            dis_loss_real = F.sum((y_v_real-1)**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_real], 'v_real')\n",
        "            \n",
        "            y_fake, fm_fake = dis_gen(x_fake)\n",
        "            dis_loss_fake = F.sum(y_fake**2) / batchsize\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_fake], 'fake')\n",
        "        \n",
        "            if self.cache_fake is not None:\n",
        "                remind, _ = dis_gen(self.cache_fake)\n",
        "                dis_loss_remind = F.sum(remind**2) / batchsize\n",
        "                dis_gen_optimizer.update(self.loss_dis, dis_gen, [dis_loss_remind], 'remind')\n",
        "\n",
        "            self.cache_fake = x_fake\n",
        "                \n",
        "            if self.cache_dis_gen is None:\n",
        "                self.cache_dis_gen = dis_gen.copy('copy')    \n",
        "            if phase >= 2 and self.cache_dis_en is None:\n",
        "                self.cache_dis_en = dis_en.copy('copy')\n",
        "                \n",
        "        dis_gen.copyparams(self.cache_dis_gen)\n",
        "        self.cache_dis_gen = None\n",
        "        if phase >= 2:\n",
        "            dis_en.copyparams(self.cache_dis_en)\n",
        "            self.cache_dis_en = None\n",
        "        \n",
        "        fake_loss = F.sum((y_fake-1)**2) / batchsize\n",
        "        if phase < 1:\n",
        "            fm_loss = F.mean_squared_error(fm_c_real, fm_fake)\n",
        "            reconst_loss = F.mean_squared_error(x_c_real, x_fake)\n",
        "            stylerec_loss = F.mean_squared_error(self.style_reconst(x_c_real),\n",
        "                                                 self.style_reconst(x_fake))\n",
        "        else:\n",
        "            fm_loss = F.mean_squared_error(fm_v_real, fm_fake)\n",
        "            reconst_loss = F.mean_squared_error(x_c_base, x_fake)\n",
        "            stylerec_loss = F.mean_squared_error(self.style_reconst(x_v_real),\n",
        "                                                 self.style_reconst(x_fake))\n",
        "            \n",
        "        if phase < 2:\n",
        "            en_fake_loss = fake_loss\n",
        "        else:\n",
        "            en_fake_loss = F.sum((hy_v_fake-1)**2) / batchsize\n",
        "\n",
        "        de_optimizer.update(self.loss_de, gen.decoder, [fake_loss, fm_loss, stylerec_loss, reconst_loss])\n",
        "        en_optimizer.update(self.loss_en, gen.encoder, [en_fake_loss, fm_loss, reconst_loss, stylerec_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifY8to6Hhsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GPU = 0\n",
        "OUT = mount+'/My Drive/result/'\n",
        "IMG_SHAPE = (128, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6bmkEnHoQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = mount+'/My Drive/picture/train_pic/**/*.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49zXHF0QHq6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(encoder=Encoder(), decoder=Decoder())\n",
        "dis_gen = DiscriminatorGen()\n",
        "dis_en = DiscriminatorEn()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6m896tHwsp",
        "colab_type": "code",
        "outputId": "3ff96871-d4db-4d7d-eb8a-54ebdfbffea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chainer.backends.cuda.get_device_from_id(GPU).use()\n",
        "gen.to_gpu()\n",
        "gen.encoder.to_gpu()\n",
        "gen.decoder.to_gpu()\n",
        "dis_gen.to_gpu()\n",
        "dis_en.to_gpu()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DiscriminatorEn at 0x7f3bacfc1b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBvJVx3H8xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_en = make_optimizer_Adam(gen.encoder, clip=False)\n",
        "opt_de = make_optimizer_Adam(gen.decoder, clip=False)\n",
        "opt_dis_gen = make_optimizer_Adam(dis_gen, clip=False)\n",
        "opt_dis_en = make_optimizer_Adam(dis_en, clip=False)\n",
        "dis_gen.t.disable_update()\n",
        "dis_en.t.disable_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3yTVR2tGV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sq(img):\n",
        "    img_width, img_height = img.size\n",
        "    sub = img_width - img_height\n",
        "    if sub == 0:\n",
        "        return img\n",
        "    crop_size = min(img.size)\n",
        "    crop_position = numpy.random.randint(0, abs(sub))\n",
        "    if sub > 0:\n",
        "        return img.crop((crop_position, 0, crop_size+crop_position, crop_size))\n",
        "    else:\n",
        "        return img.crop((0, crop_position, crop_size, crop_size+crop_position))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr27raJaIDGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_convert(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    width, height = img.size\n",
        "    image_trans_trigger = numpy.random.randn(2)\n",
        "    \n",
        "    img = crop_sq(img)\n",
        "        \n",
        "    if image_trans_trigger[0] > 0:\n",
        "        img = ImageOps.mirror(img)\n",
        "        \n",
        "    if image_trans_trigger[1] > 0:\n",
        "        img = img.resize(IMG_SHAPE)\n",
        "    else:\n",
        "        img = img.resize((IMG_SHAPE[0]+16, IMG_SHAPE[1]+16))\n",
        "        crop_position = numpy.random.randint(0, 16, 2)\n",
        "        img = img.crop((crop_position[0],\n",
        "                       crop_position[1],\n",
        "                       crop_position[0]+IMG_SHAPE[0],\n",
        "                       crop_position[1]+IMG_SHAPE[1]))\n",
        "\n",
        "    real_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    img = img.filter(ImageFilter.GaussianBlur(1.0))\n",
        "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
        "    img = img.quantize(8, kmeans=True).convert('RGB')\n",
        "    \n",
        "    base_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    out_array = numpy.concatenate((real_array[None, :, :, :],\n",
        "                                   base_array[None, :, :, :]), axis=0)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1l6Zuw9IHKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_resize(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    img = crop_sq(img)\n",
        "      \n",
        "    img = img.resize(IMG_SHAPE)\n",
        "    \n",
        "    real_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    img = img.filter(ImageFilter.GaussianBlur(1.0))\n",
        "    img = img.filter(ImageFilter.SMOOTH_MORE)\n",
        "    img = img.quantize(8, kmeans=True).convert('RGB')\n",
        "    \n",
        "    base_array = numpy.asarray(img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    out_array = numpy.concatenate((real_array[None, :, :, :],\n",
        "                                   base_array[None, :, :, :]), axis=0)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xynzSKG4IKYP",
        "colab_type": "code",
        "outputId": "3988828d-a2c3-4652-98e7-a3439e4d9704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "image_files = glob.glob(dataset, recursive=True)\n",
        "print('{} contains {} image files'\n",
        "      .format(dataset, len(image_files)))\n",
        "img_dataset = chainer.datasets.ImageDataset(paths=image_files)\n",
        "\n",
        "train = chainer.datasets.TransformDataset(img_dataset, img_convert)\n",
        "sample = chainer.datasets.TransformDataset(img_dataset, img_resize)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/picture/train_pic/**/*.jpg contains 43438 image files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsS_Vf0KIOQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_c_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)\n",
        "train_v_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToPAI31hIQoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "    models=(gen, dis_gen, dis_en),\n",
        "    iterator={'main': train_v_iter, 'style': train_c_iter},\n",
        "    optimizer={'de': opt_de, 'en': opt_en, 'dis_gen': opt_dis_gen, 'dis_en': opt_dis_en},\n",
        "    device=GPU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5phMAnIWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = training.Trainer(updater, (iteration, 'iteration'), out=OUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqcOBLPUIaSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o2TkGb1Icnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(out_generated_image(gen,\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True),\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True)),\n",
        "               trigger=snapshot_interval)\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'dis_en/v_loss', 'en/loss', 'dis_gen/c_real', 'dis_gen/fake', 'de/loss'\n",
        "    ]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=update_interval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNdaVJ4gy9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if phase > 0:\n",
        "    chainer.serializers.load_npz(OUT+'mane_de.npz', gen.decoder, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_dis_gen.npz', dis_gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_opt_de.npz', opt_de, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_opt_dis_gen.npz', opt_dis_gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_en.npz', gen.encoder, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_opt_en.npz', opt_en, strict=False)\n",
        "    \n",
        "if phase > 2:\n",
        "    chainer.serializers.load_npz(OUT+'mane_dis_en.npz', dis_en, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'mane_opt_dis_en.npz', opt_dis_en, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epn0ZUHekKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chainer.serializers.save_npz(OUT+'mane_de.npz', gen.decoder)\n",
        "chainer.serializers.save_npz(OUT+'mane_dis_gen.npz', dis_gen)\n",
        "chainer.serializers.save_npz(OUT+'mane_opt_de.npz', opt_de)\n",
        "chainer.serializers.save_npz(OUT+'mane_opt_dis_gen.npz', opt_dis_gen)\n",
        "chainer.serializers.save_npz(OUT+'mane_en.npz', gen.encoder)\n",
        "chainer.serializers.save_npz(OUT+'mane_opt_en.npz', opt_en)\n",
        "    \n",
        "if phase >= 2:\n",
        "    chainer.serializers.save_npz(OUT+'mane_dis_en.npz', dis_en)\n",
        "    chainer.serializers.save_npz(OUT+'mane_opt_dis_en.npz', opt_dis_en)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}