{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GauGANnanteHEdemonai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "btB-TcgGrzsu",
        "colab_type": "code",
        "outputId": "e61dfa58-b76b-4e75-b658-8d502501f9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 15 11:50:56 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    18W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2LONgCSHkKo",
        "colab_type": "code",
        "outputId": "b3fcadd0-9144-4359-8ac1-2131db61c6ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "iteration = 5\n",
        "batchsize = 128\n",
        "snapshot_interval = 5\n",
        "display_interval = snapshot_interval\n",
        "update_interval = snapshot_interval\n",
        "phase = 0\n",
        "IMG_SHAPE = (32, 32)\n",
        "this_resolution_is_start = False\n",
        "this_resolution_is_fin = False\n",
        "GPU = 0.1\n",
        "'''\n",
        "0- : all decoder training.\n",
        "1- : trans encoding.(hold)\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n0- : all decoder training.\\n1- : trans encoding.(hold)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlnDuRC0v0Ru",
        "colab_type": "code",
        "outputId": "252abe1e-f457-4d58-b822-a47d8d091d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mount = '/content/gdrive'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(mount)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NifY8to6Hhsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUT = mount+'/My Drive/result/'\n",
        "dataset = mount+'/My Drive/picture/train_pic/**/*.jpg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGFtpuE3Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import math\n",
        "import glob\n",
        "from PIL import Image, ImageOps, ImageChops, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy\n",
        "\n",
        "import chainer\n",
        "from chainer import training, backend, Variable\n",
        "from chainer.training import extensions\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "import chainer.backends.cuda\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGloZh9xEu9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(size):\n",
        "    return F.gaussian(cupy.zeros(size, dtype=cupy.float32),\n",
        "                     cupy.ones(size, dtype=cupy.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9P2_zkbFjeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_optimizer_Adam(model, alpha=1e-4, beta1=0.9, clip=True):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1)\n",
        "    optimizer.setup(model)\n",
        "    if clip:\n",
        "        optimizer.add_hook(chainer.optimizer_hooks.GradientClipping(1.))\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLOB11ezFNMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def out_generated_image(gen, c_iter, v_iter):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        clear_output()\n",
        "        xp = gen.xp\n",
        "        c_base = xp.asarray(c_iter.next())\n",
        "        z_c = Variable(c_base[:,1])/255. *2. -1.\n",
        "        if phase < 1:\n",
        "            v_base = c_base[:,0]\n",
        "        else:\n",
        "            v_base = xp.asarray(v_iter.next())[:,0]\n",
        "        z_v = Variable(v_base)/255. *2. -1.\n",
        "        \n",
        "        with chainer.using_config('train', False):\n",
        "            x, zs, seg, tx = gen(z_c, z_v)\n",
        "            \n",
        "        x = F.transpose(F.reshape(x, (-1, 3)+IMG_SHAPE), (0, 2, 3, 1))\n",
        "        x = chainer.backends.cuda.to_cpu(x.array)\n",
        "        zs = chainer.backends.cuda.to_cpu(zs.array)\n",
        "        seg = F.transpose(F.reshape(seg, (-1, 3)+IMG_SHAPE), (0, 2, 3, 1))\n",
        "        seg = chainer.backends.cuda.to_cpu(seg.array)\n",
        "        tx = F.transpose(F.reshape(tx, (-1, 3)+IMG_SHAPE), (0, 2, 3, 1))\n",
        "        tx = chainer.backends.cuda.to_cpu(tx.array)\n",
        "        \n",
        "        plt.figure(figsize=(16, 24))\n",
        "        \n",
        "        for i, img in enumerate(xp.asnumpy(c_base[:,1])):\n",
        "            plt.subplot(6, 4, i+1).axis('off')\n",
        "            plt.subplot(6, 4, i+1).imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "            \n",
        "        for i, img in enumerate(seg):\n",
        "            plt.subplot(6, 4, i+5).axis('off')\n",
        "            plt.subplot(6, 4, i+5).imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "            \n",
        "        for i, img in enumerate(x):\n",
        "            plt.subplot(6, 4, i+9).axis('off')\n",
        "            plt.subplot(6, 4, i+9).imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "\n",
        "        for i, img in enumerate(tx):\n",
        "            plt.subplot(6, 4, i+13).axis('off')\n",
        "            plt.subplot(6, 4, i+13).imshow(Image.fromarray(numpy.uint8((img+1.)/2. *255.)))\n",
        "            \n",
        "        ax = plt.subplot2grid((6, 4), (4, 0), colspan=4)\n",
        "        for i, zs in enumerate(zs):\n",
        "            ax.plot(zs,\n",
        "                    marker='.',\n",
        "                    linestyle='None',\n",
        "                    label='z({})'.format(i),\n",
        "                    color='rgbk'[i])\n",
        "            ax.legend()\n",
        "            \n",
        "        for i, img in enumerate(xp.asnumpy(v_base)):\n",
        "            plt.subplot(6, 4, i+21).axis('off')\n",
        "            plt.subplot(6, 4, i+21).imshow(Image.fromarray(numpy.uint8(img).transpose((1, 2, 0))))\n",
        "        \n",
        "        plt.show()\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzW6AWqpGp8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscriminatorGen(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=5e-1):\n",
        "        super(DiscriminatorGen, self).__init__()\n",
        "        \n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.c0_0 = L.Convolution2D(3, 64, 1, 1, 0, initialW=w)\n",
        "            self.bn0_0 = L.BatchNormalization(64)\n",
        "            self.c0_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn0_1 = L.BatchNormalization(64)\n",
        "            self.c0_2 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.bn0_2 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c1_0 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_0 = L.BatchNormalization(128)\n",
        "            self.c1_1 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_1 = L.BatchNormalization(128)\n",
        "            self.c1_2 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.bn1_2 = L.BatchNormalization(128)\n",
        "            self.c1_3 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn1_3 = L.BatchNormalization(128)\n",
        "            self.c1_4 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.bn1_4 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.c2_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn2_0 = L.BatchNormalization(256)\n",
        "            self.c2_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.bn2_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.c3_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn3_0 = L.BatchNormalization(512)\n",
        "            self.c3_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn3_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.c4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn4_0 = L.BatchNormalization(512)\n",
        "            self.c4_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn4_1= L.BatchNormalization(512)\n",
        "            \n",
        "            self.c5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn5_0 = L.BatchNormalization(512)\n",
        "            self.c5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.bn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.l_m = L.Linear(None, 1024, initialW=w)\n",
        "            self.t = L.Linear(None, 64*16, initialW=None)\n",
        "            \n",
        "            self.l_o = L.Linear(None, 1, initialW=w)\n",
        "    \n",
        "    def __call__(self, z):\n",
        "        batchsize = z.shape[0]\n",
        "        x = F.leaky_relu(self.bn0_0(self.c0_0(self.gaussian(z))))\n",
        "        x = F.leaky_relu(self.bn0_1(self.c0_1(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn0_2(self.c0_2(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 128\n",
        "        x = F.average_pooling_2d(x, 2)\n",
        "        f = F.tanh(self.bn1_0(self.c1_0(x)))\n",
        "        g = F.tanh(self.bn1_1(self.c1_1(x)))\n",
        "        h = F.tanh(self.bn1_2(self.c1_2(x)))\n",
        "        x = F.leaky_relu(self.self_attention(x, f, g, h))\n",
        "        x = F.leaky_relu(self.bn1_3(self.c1_3(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn1_4(self.c1_4(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 64\n",
        "        x = F.average_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn2_0(self.c2_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn2_1(self.c2_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 32\n",
        "        x = F.average_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn3_0(self.c3_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn3_1(self.c3_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 16\n",
        "        x = F.average_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn4_0(self.c4_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn4_1(self.c4_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        # 8\n",
        "        x = F.average_pooling_2d(x, 2)\n",
        "        x = F.leaky_relu(self.bn5_0(self.c5_0(self.gaussian(x))))\n",
        "        _x = F.leaky_relu(self.bn5_1(self.c5_1(self.gaussian(x))))\n",
        "        x = F.concat((x, _x))\n",
        "        x = self.gap(x)\n",
        "        # 4\n",
        "        \n",
        "        h_m = F.leaky_relu(self.l_m(x))\n",
        "        m = F.reshape(self.t(h_m), (batchsize, 64, 16))\n",
        "        o_b = self.minibatch_discrimination(m)\n",
        "        \n",
        "        x = F.concat((h_m, o_b), axis=1)\n",
        "        \n",
        "        return self.l_o(x), h_m\n",
        "\n",
        "    def minibatch_discrimination(self, m):\n",
        "        batchsize = m.shape[0]\n",
        "        m = F.expand_dims(m, 3)\n",
        "        m_T = F.transpose(m, (3, 1, 2, 0))\n",
        "        m, m_T = F.broadcast(m, m_T)\n",
        "        norm = F.sum(F.absolute_error(m, m_T), axis=2)\n",
        "        eraser = F.broadcast_to(cupy.eye(batchsize, dtype=cupy.float32).reshape((batchsize, 1, batchsize)), norm.shape)\n",
        "        c_b = F.exp(-(norm + 1e6 * eraser))\n",
        "        o_b = F.sum(c_b, axis=2)\n",
        "        return o_b\n",
        "        \n",
        "    def gaussian(self, x):\n",
        "        return F.gaussian(x, cupy.ones(x.shape, dtype=cupy.float32)*2e-2)\n",
        "    \n",
        "    def self_attention(self, x, f, g, h, gamma=0.01):\n",
        "        f_shape = f.shape\n",
        "        g_shape = g.shape\n",
        "        h_shape = h.shape\n",
        "        \n",
        "        attention_map = f*F.transpose(g, (0, 1, 3, 2))\n",
        "        feature_map = h*attention_map\n",
        "        return F.add(x, feature_map*gamma)\n",
        "    \n",
        "    def gap(self, x):\n",
        "        return F.average_pooling_2d(x, x.shape[-2:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEhOWEuFqEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, wscale=5e-1):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.HeNormal(wscale)\n",
        "            \n",
        "            self.txc0_0 = L.Convolution2D(3, 64, 1, 1, 0, initialW=w)\n",
        "            self.txcbn0_0 = L.BatchNormalization(64)\n",
        "            self.txc0_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.txcbn0_1 = L.BatchNormalization(64)\n",
        "            self.txc0_2 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.txcbn0_2 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.txc1_0 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.txcbn1_0 = L.BatchNormalization(128)\n",
        "            self.txc1_1 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.txcbn1_1 = L.BatchNormalization(128)\n",
        "            self.txc1_2 = L.Convolution2D(None, 128, 1, 1, 0, initialW=w)\n",
        "            self.txcbn1_2 = L.BatchNormalization(128)\n",
        "            self.txc1_3 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.txcbn1_3 = L.BatchNormalization(128)\n",
        "            self.txc1_4 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.txcbn1_4 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.txc2_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.txcbn2_0 = L.BatchNormalization(256)\n",
        "            self.txc2_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.txcbn2_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.txc3_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn3_0 = L.BatchNormalization(512)\n",
        "            self.txc3_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn3_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.txc4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn4_0 = L.BatchNormalization(512)\n",
        "            self.txc4_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn4_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.txc5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn5_0 = L.BatchNormalization(512)\n",
        "            self.txc5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.txcbn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.txl1 = L.Linear(None, 1024, initialW=w)\n",
        "            self.txlbn1 = L.BatchNormalization(1024)\n",
        "            self.txl2 = L.Linear(None, 1024, initialW=w)\n",
        "            self.txlbn2 = L.BatchNormalization(1024)\n",
        "            self.txl3 = L.Linear(None, 1024, initialW=w)\n",
        "            \n",
        "            self.txbtm = L.Linear(None, 512*8*8, initialW=w)\n",
        "            \n",
        "            self.enc0_0 = L.Convolution2D(3, 64, 1, 1, 0, initialW=w)\n",
        "            self.enbnc0_0 = L.BatchNormalization(64)\n",
        "            self.enc0_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.enbnc0_1 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.enc1_0 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.enbnc1_0 = L.BatchNormalization(256)\n",
        "            self.enc1_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.enbnc1_1 = L.BatchNormalization(128)\n",
        "            \n",
        "            self.enc2_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.enbnc2_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.enc3_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.enbnc3_1 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.enc_gamma = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.enbnc_gamma = L.BatchNormalization(256)\n",
        "            self.enc_beta = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.enbnc_beta = L.BatchNormalization(256)\n",
        "            \n",
        "            self.de5_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn5_0 = L.BatchNormalization(512)\n",
        "            self.de5_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn5_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            self.de4_0 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn4_0 = L.BatchNormalization(512)\n",
        "            self.de4_1 = L.Convolution2D(None, 512, 3, 1, 1, initialW=w)\n",
        "            self.debn4_1 = L.BatchNormalization(512)\n",
        "            \n",
        "            ''' ---64px image training---\n",
        "            self.de3_0 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.debn3_0 = L.BatchNormalization(256)\n",
        "            self.de3_1 = L.Convolution2D(None, 256, 3, 1, 1, initialW=w)\n",
        "            self.debn3_1 = L.BatchNormalization(256)\n",
        "            '''\n",
        "            ''' ---128px image training---\n",
        "            self.de2_0 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.debn2_0 = L.BatchNormalization(128)\n",
        "            self.de2_1 = L.Convolution2D(None, 128, 3, 1, 1, initialW=w)\n",
        "            self.debn2_1 = L.BatchNormalization(128)\n",
        "            '''\n",
        "            \n",
        "            self.de2_2 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.debn2_2 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.de2_3 = L.Convolution2D(None, 256, 1, 1, 0, initialW=w)\n",
        "            self.debn2_3 = L.BatchNormalization(256)\n",
        "            \n",
        "            self.de1_0 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.debn1_0 = L.BatchNormalization(64)\n",
        "            self.de1_1 = L.Convolution2D(None, 64, 3, 1, 1, initialW=w)\n",
        "            self.debn1_1 = L.BatchNormalization(64)\n",
        "            \n",
        "            self.c_out = L.Convolution2D(None, 3, 1, 1, 0, initialW=w)\n",
        "    \n",
        "    def __call__(self, x, style):        \n",
        "        batch = x.shape[0]\n",
        "        z = self.tx_encode(style)\n",
        "        g, b = self.seg_generate(x)\n",
        "        h = self.decode(z, batch)\n",
        "        \n",
        "        seg_out = self.out_generate(F.add(g, b), batch)\n",
        "        tx_out = self.out_generate(h, batch)\n",
        "        h = self.out_generate(h, batch, g=g, b=b)\n",
        "        return h, z, seg_out, tx_out\n",
        "            \n",
        "    def self_attention(self, x, f, g, h, gamma=0.01):\n",
        "        f_shape = f.shape\n",
        "        g_shape = g.shape\n",
        "        h_shape = h.shape\n",
        "        \n",
        "        attention_map = f*F.transpose(g, (0, 1, 3, 2))\n",
        "        feature_map = h*attention_map\n",
        "        return F.add(x, feature_map*gamma)\n",
        "    \n",
        "    def gap(self, x):\n",
        "        return F.average_pooling_2d(x, x.shape[-2:])\n",
        "    \n",
        "    def spade(self, feature, gamma, beta):\n",
        "        x = feature*gamma\n",
        "        x = F.add(x, beta)\n",
        "        return x\n",
        "    \n",
        "    def gaussian(self, shape):\n",
        "        return F.gaussian(cupy.zeros(shape, dtype=cupy.float32),\n",
        "                           cupy.ones(shape, dtype=cupy.float32))\n",
        "    \n",
        "    def tx_encode(self, s):\n",
        "        z = F.leaky_relu(self.txcbn0_0(self.txc0_0(s)))\n",
        "        z = F.leaky_relu(self.txcbn0_1(self.txc0_1(z)))\n",
        "        _z = F.leaky_relu(self.txcbn0_2(self.txc0_2(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 128\n",
        "        z = F.average_pooling_2d(z, 2)\n",
        "        f = F.tanh(self.txcbn1_0(self.txc1_0(z)))\n",
        "        g = F.tanh(self.txcbn1_1(self.txc1_1(z)))\n",
        "        h = F.tanh(self.txcbn1_2(self.txc1_2(z)))\n",
        "        z = F.leaky_relu(self.self_attention(z, f, g, h))\n",
        "        z = F.leaky_relu(self.txcbn1_3(self.txc1_3(z)))\n",
        "        _z = F.leaky_relu(self.txcbn1_4(self.txc1_4(z)))\n",
        "        z = F.concat((z,_z))\n",
        "        # 64\n",
        "        z = F.average_pooling_2d(z, 2)\n",
        "        z = F.leaky_relu(self.txcbn2_0(self.txc2_0(z)))\n",
        "        _z = F.leaky_relu(self.txcbn2_1(self.txc2_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 32\n",
        "        z = F.average_pooling_2d(z, 2)\n",
        "        z = F.leaky_relu(self.txcbn3_0(self.txc3_0(z)))\n",
        "        _z = F.leaky_relu(self.txcbn3_1(self.txc3_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 16\n",
        "        z = F.average_pooling_2d(z, 2)\n",
        "        z = F.leaky_relu(self.txcbn4_0(self.txc4_0(z)))\n",
        "        _z = F.leaky_relu(self.txcbn4_1(self.txc4_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 8\n",
        "        z = F.average_pooling_2d(z, 2)\n",
        "        z = F.leaky_relu(self.txcbn5_0(self.txc5_0(z)))\n",
        "        _z = F.leaky_relu(self.txcbn5_1(self.txc5_1(z)))\n",
        "        z = F.concat((z, _z))\n",
        "        # 4\n",
        "        z = self.gap(z)\n",
        "        \n",
        "        z = F.leaky_relu(self.txlbn1(self.txl1(z)))\n",
        "        z = F.leaky_relu(self.txlbn2(self.txl2(z)))\n",
        "        z = F.tanh(self.txl3(z))\n",
        "        return z\n",
        "    \n",
        "    def seg_generate(self, x):\n",
        "        s = F.leaky_relu(self.enbnc0_0(self.enc0_0(x)))\n",
        "        f = F.leaky_relu(self.enbnc0_1(self.enc0_1(s)))\n",
        "        # 128\n",
        "        s = F.space2depth(s, 2)\n",
        "        f = F.average_pooling_2d(f, 2)\n",
        "        s = F.tanh(self.enbnc1_0(self.enc1_0(s)))\n",
        "        f = F.leaky_relu(self.enbnc1_1(self.enc1_1(f)))\n",
        "        # 64\n",
        "        f = F.average_pooling_2d(f, 2)\n",
        "        f = F.leaky_relu(self.enbnc2_1(self.enc2_1(f)))\n",
        "        # 32\n",
        "        f = F.average_pooling_2d(f, 2)\n",
        "        f = F.leaky_relu(self.enbnc3_1(self.enc3_1(f)))\n",
        "        # 32\n",
        "        f = F.unpooling_2d(f, 4, cover_all=False)\n",
        "        f = F.leaky_relu(f*s)\n",
        "        \n",
        "        g = F.tanh(self.enbnc_gamma(self.enc_gamma(f)))\n",
        "        b = F.tanh(self.enbnc_beta(self.enc_beta(f)))\n",
        "        return g, b\n",
        "    \n",
        "    def decode(self, z, batch):\n",
        "        h = F.leaky_relu(F.reshape(self.txbtm(z), (-1, 512, 8, 8)))\n",
        "        n = self.gaussian((batch, 1, 8, 8))\n",
        "        h = F.concat((h, n))\n",
        "        h = F.leaky_relu(self.debn5_0(self.de5_0(h)))\n",
        "        n = self.gaussian((batch, 1, 8, 8))\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.leaky_relu(self.debn5_1(self.de5_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        # 8\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian((batch, 1, 16, 16))\n",
        "        h = F.concat((h, n))\n",
        "        h = F.leaky_relu(self.debn4_0(self.de4_0(h)))\n",
        "        n = self.gaussian((batch, 1, 16, 16))\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.leaky_relu(self.debn4_1(self.de4_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        # 16\n",
        "        ''' ---64px image training---\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian((batch, 1, 32, 32))\n",
        "        h = F.concat((h, n))\n",
        "        h = F.leaky_relu(self.debn3_0(self.de3_0(h)))\n",
        "        n = self.gaussian((batch, 1, 32, 32))\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.leaky_relu(self.debn3_1(self.de3_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        '''\n",
        "        '''---128px image training---\n",
        "        # 32\n",
        "        h = F.depth2space(h, 2)\n",
        "        n = self.gaussian((batch, 1, 64, 64))\n",
        "        h = F.concat((h, n))\n",
        "        h = F.leaky_relu(self.debn2_0(self.de2_0(h)))\n",
        "        n = self.gaussian((batch, 1, 64, 64))\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.leaky_relu(self.debn2_1(self.de2_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        '''\n",
        "        h = F.leaky_relu(self.debn2_2(self.de2_2(h)))\n",
        "        return h\n",
        "    \n",
        "    def out_generate(self, tx, batch, g=None, b=None):\n",
        "        if g is not None and b is not None:\n",
        "            h = F.leaky_relu(self.spade(tx, g, b))\n",
        "            h = F.leaky_relu(self.debn2_3(self.de2_3(h)))\n",
        "            h = F.depth2space(h, 2)\n",
        "        else:\n",
        "            h = F.depth2space(tx, 2)\n",
        "            \n",
        "        n = self.gaussian((batch, 1)+IMG_SHAPE)\n",
        "        h = F.concat((h, n))\n",
        "        h = F.leaky_relu(self.debn1_0(self.de1_0(h)))\n",
        "        n = self.gaussian((batch, 1)+IMG_SHAPE)\n",
        "        _h = F.concat((h, n))\n",
        "        _h = F.leaky_relu(self.debn1_1(self.de1_1(_h)))\n",
        "        h = F.concat((h, _h))\n",
        "        \n",
        "        h = F.tanh(self.c_out(h))\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkPjj5gHE76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis_gen = kwargs.pop('models')\n",
        "        self.cache_dis_gen = None\n",
        "        self.k = 5\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def loss_dis(self, dis, loss, opt):\n",
        "        chainer.report({opt: loss}, dis)\n",
        "        return loss\n",
        "    \n",
        "    def loss_gen(self, gen, losses):\n",
        "        loss = F.sum(losses) \n",
        "        chainer.report({'loss': loss,\n",
        "                        'fake': losses[0],\n",
        "                        'rec': losses[2],\n",
        "                        'tx': losses[3],\n",
        "                        'seg': losses[5],\n",
        "                        'z': losses[-1]}, gen)\n",
        "        return loss\n",
        "\n",
        "    def update_core(self):\n",
        "        gen_optimizer = self.get_optimizer('gen')\n",
        "        dis_gen_optimizer = self.get_optimizer('dis_gen')\n",
        "        \n",
        "        gen, dis_gen = self.gen, self.dis_gen\n",
        "        main_iter = self.get_iterator('main')\n",
        "        style_iter = self.get_iterator('style')\n",
        "        \n",
        "        for k in range(self.k):\n",
        "            batch_c = main_iter.next()\n",
        "            batchsize = len(batch_c)\n",
        "            x_c = Variable(self.converter(batch_c, device=self.device)) /255. *2. -1.\n",
        "            x_c_real = x_c[:,0]\n",
        "            x_c_base = x_c[:,1]\n",
        "\n",
        "            batch_v = style_iter.next()\n",
        "            x_v_real = Variable(self.converter(batch_v, device=self.device))[:,0] /255. *2. -1.\n",
        "            \n",
        "            if phase >= 1:\n",
        "                x_fake, z, seg_fake, tx_fake = gen(x_c_base, x_v_real)\n",
        "            else:\n",
        "                x_fake, z, seg_fake, tx_fake = gen(x_c_base, x_c_real)\n",
        "            \n",
        "            y_c_real, fm_c_real = dis_gen(x_c_real)\n",
        "            y_seg_fake, fm_seg_fake = dis_gen(seg_fake)\n",
        "            dis_loss_real = F.mean((y_c_real-1)**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_real, 'c_real')\n",
        "            dis_loss_seg = F.mean(y_seg_fake**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_seg, 'fake_seg')\n",
        "            \n",
        "            if k == self.k-1:\n",
        "                seg_loss = F.mean((y_seg_fake-1)**2)\n",
        "                seg_rec_loss = F.mean_squared_error(x_c_base, seg_fake)**2\n",
        "                seg_fm_loss = F.mean_squared_error(fm_c_real, fm_seg_fake)\n",
        "                \n",
        "            del y_seg_fake, fm_seg_fake\n",
        "            \n",
        "            y_v_real, fm_v_real = dis_gen(x_v_real)\n",
        "            y_tx_fake, fm_tx_fake = dis_gen(tx_fake)\n",
        "            dis_loss_real = F.mean((y_v_real-1)**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_real, 'v_real')\n",
        "            dis_loss_tx = F.mean(y_tx_fake**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_tx, 'fake_tx')\n",
        "            \n",
        "            if k == self.k-1:\n",
        "                tx_loss = F.mean((y_tx_fake-1)**2)\n",
        "                if phase < 1:\n",
        "                    tx_fm_loss = F.mean_squared_error(fm_c_real, fm_tx_fake)\n",
        "                else:\n",
        "                    tx_fm_loss = F.mean_squared_error(fm_v_real, fm_tx_fake)\n",
        "                \n",
        "            del y_tx_fake, fm_tx_fake\n",
        "            \n",
        "            y_fake, fm_fake = dis_gen(x_fake)\n",
        "            if phase < 1:\n",
        "                dis_loss_real = F.mean((y_c_real-1)**2)\n",
        "            else:\n",
        "                dis_loss_real = F.mean((y_v_real-1)**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_real, 'repeat')\n",
        "            dis_loss_fake = F.mean(y_fake**2)\n",
        "            dis_gen_optimizer.update(self.loss_dis, dis_gen, dis_loss_fake, 'fake')\n",
        "            \n",
        "            if k == self.k-1:\n",
        "                fake_loss = F.mean((y_fake-1)**2)\n",
        "                reconst_loss = F.mean_squared_error(x_c_base, x_fake)**2\n",
        "                if phase < 1:\n",
        "                    fm_loss = F.mean_squared_error(fm_c_real, fm_fake)\n",
        "                else:\n",
        "                    fm_loss = F.mean_squared_error(fm_v_real, fm_fake)\n",
        "                    \n",
        "            del y_fake, fm_fake, y_v_real, fm_v_real, y_c_real, fm_c_real\n",
        "                \n",
        "            if self.cache_dis_gen is None:\n",
        "                self.cache_dis_gen = dis_gen.copy('copy')\n",
        "                \n",
        "        dis_gen.copyparams(self.cache_dis_gen)\n",
        "        self.cache_dis_gen = None\n",
        "        \n",
        "        z_loss = F.gaussian_kl_divergence(cupy.zeros(z.shape, dtype=cupy.float32), z)*1e-2\n",
        "\n",
        "        gen_optimizer.update(self.loss_gen, gen, F.stack((fake_loss,\n",
        "                                                          fm_loss,\n",
        "                                                          reconst_loss,\n",
        "                                                          tx_loss,\n",
        "                                                          tx_fm_loss,\n",
        "                                                          seg_loss,\n",
        "                                                          seg_fm_loss,\n",
        "                                                          seg_rec_loss,\n",
        "                                                          z_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49zXHF0QHq6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator()\n",
        "dis_gen = DiscriminatorGen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6m896tHwsp",
        "colab_type": "code",
        "outputId": "117122f6-81c6-47ca-fcae-2fc4811ba475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chainer.backends.cuda.get_device_from_id(GPU).use()\n",
        "gen.to_gpu()\n",
        "dis_gen.to_gpu()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.DiscriminatorGen at 0x7f2b9b5b8550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwBvJVx3H8xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = make_optimizer_Adam(gen, clip=False)\n",
        "opt_dis_gen = make_optimizer_Adam(dis_gen, clip=False)\n",
        "dis_gen.t.disable_update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE3yTVR2tGV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crop_sq(img):\n",
        "    img_width, img_height = img.size\n",
        "    sub = img_width - img_height\n",
        "    if sub == 0:\n",
        "        return img\n",
        "    crop_size = min(img.size)\n",
        "    crop_position = numpy.random.randint(0, abs(sub))\n",
        "    if sub > 0:\n",
        "        return img.crop((crop_position, 0, crop_size+crop_position, crop_size))\n",
        "    else:\n",
        "        return img.crop((0, crop_position, crop_size, crop_size+crop_position))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr27raJaIDGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_convert(img_array):\n",
        "    img = Image.fromarray(numpy.uint8(img_array.transpose(1, 2, 0))).convert('RGB')\n",
        "    image_trans_trigger = numpy.random.randn(2)\n",
        "    \n",
        "    real_img = crop_sq(img)\n",
        "    base_img = crop_sq(img)\n",
        "        \n",
        "    if image_trans_trigger[0] > 0:\n",
        "        real_img = ImageOps.mirror(real_img)\n",
        "        \n",
        "    if image_trans_trigger[1] > 0:\n",
        "        real_img = real_img.resize(IMG_SHAPE)\n",
        "    else:\n",
        "        real_img = real_img.resize((IMG_SHAPE[0]+IMG_SHAPE[0]//8, IMG_SHAPE[1]+IMG_SHAPE[1]//8))\n",
        "        crop_position = numpy.random.randint(0, IMG_SHAPE[0]//8, 2)\n",
        "        real_img = real_img.crop((crop_position[0],\n",
        "                                  crop_position[1],\n",
        "                                  crop_position[0]+IMG_SHAPE[0],\n",
        "                                  crop_position[1]+IMG_SHAPE[1]))\n",
        "\n",
        "    real_array = numpy.asarray(real_img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "        \n",
        "    base_img = base_img.resize(IMG_SHAPE)\n",
        "    \n",
        "    base_img = base_img.filter(ImageFilter.GaussianBlur(1.0))\n",
        "    base_img = base_img.filter(ImageFilter.SMOOTH_MORE)\n",
        "    base_img = base_img.quantize(8, kmeans=True).convert('RGB')\n",
        "    \n",
        "    base_array = numpy.asarray(base_img, dtype=numpy.float32).transpose(2, 0, 1)\n",
        "    \n",
        "    out_array = numpy.concatenate((real_array[None, :, :, :],\n",
        "                                   base_array[None, :, :, :]), axis=0)\n",
        "    return out_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xynzSKG4IKYP",
        "colab_type": "code",
        "outputId": "7932e9ce-4363-48d5-aad7-89995637b501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "image_files = glob.glob(dataset, recursive=True)\n",
        "print('{} contains {} image files'\n",
        "      .format(dataset, len(image_files)))\n",
        "img_dataset = chainer.datasets.ImageDataset(paths=image_files)\n",
        "\n",
        "train = chainer.datasets.TransformDataset(img_dataset, img_convert)\n",
        "sample = chainer.datasets.TransformDataset(img_dataset, img_convert)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/picture/train_pic/**/*.jpg contains 1062026 image files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsS_Vf0KIOQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_c_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)\n",
        "train_v_iter = chainer.iterators.SerialIterator(train, batchsize, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToPAI31hIQoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "    models=(gen, dis_gen),\n",
        "    iterator={'main': train_v_iter, 'style': train_c_iter},\n",
        "    optimizer={'gen': opt_gen, 'dis_gen': opt_dis_gen},\n",
        "    device=GPU)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5phMAnIWQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = training.Trainer(updater, (iteration, 'iteration'), out=OUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqcOBLPUIaSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o2TkGb1Icnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(out_generated_image(gen,\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True),\n",
        "                                   chainer.iterators.SerialIterator(sample, 4, shuffle=True)),\n",
        "               trigger=snapshot_interval)\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'dis_gen/c_real', 'gen/loss', 'gen/fake', 'gen/seg', 'gen/tx', 'gen/z'\n",
        "    ]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=update_interval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNdaVJ4gy9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if phase > 0:\n",
        "    chainer.serializers.load_npz(OUT+'korosu_gen.npz', gen, strict=False)\n",
        "    chainer.serializers.load_npz(OUT+'korosu_dis_gen.npz', dis_gen, strict=False)\n",
        "    if not this_resolution_is_start:\n",
        "        chainer.serializers.load_npz(OUT+'korosu_opt_gen.npz', opt_gen, strict=False)\n",
        "        chainer.serializers.load_npz(OUT+'korosu_opt_dis_gen.npz', opt_dis_gen, strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HefwIhPNI4hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epn0ZUHekKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if this_resolution_is_fin:\n",
        "    del gen.de2_2, gen.debn2_2\n",
        "chainer.serializers.save_npz(OUT+'korosu_gen.npz', gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_dis_gen.npz', dis_gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_opt_gen.npz', opt_gen)\n",
        "chainer.serializers.save_npz(OUT+'korosu_opt_dis_gen.npz', opt_dis_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzIsneAgeTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}